{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/QNLP/blob/main/data_cleanup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleanup Code\n",
        "\n",
        "This notebook is setup to read the uspantekan data files (.conllu), cleanup the non-ASCII characters in the name of the file. It extract the relevant lines that contain the Spanish translations of the text along with the line IDs, from different files, adds a label (0, 1...) to indicate file origin. Finally, all the lines are randomized and split into train, test and dev files to be fed to the lambeq pipeline.\n",
        "\n",
        "Currently, we are working on `Bailes_de_Uspantán.conllu` and `Bailes_de_Uspantan.conllu`."
      ],
      "metadata": {
        "id": "mAZgqijexlJo"
      },
      "id": "mAZgqijexlJo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0122f600-0f91-41b6-8739-51ef268a05bc",
      "metadata": {
        "id": "0122f600-0f91-41b6-8739-51ef268a05bc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir()\n",
        "os.chdir(\".\")\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename files to remove non-ASCII characters:"
      ],
      "metadata": {
        "id": "ml0QHr_BczNb"
      },
      "id": "ml0QHr_BczNb"
    },
    {
      "cell_type": "code",
      "source": [
        "# ASCII converter to rename files with non-ASCII characters:\n",
        "!rename -n 's/[^\\x00-\\x7F]//g' *\n"
      ],
      "metadata": {
        "id": "As3mF61ZIRrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda36216-9858-4829-faa6-cdb3b12723bf"
      },
      "id": "As3mF61ZIRrt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rename(Bailes_de_Uspantán.conllu, Bailes_de_Uspantan.conllu)\n",
            "rename(Educación_en_la_comunidad.conllu, Educacion_en_la_comunidad.conllu)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a9d2e3-9155-49eb-bf4e-17246fefb609",
      "metadata": {
        "id": "50a9d2e3-9155-49eb-bf4e-17246fefb609"
      },
      "outputs": [],
      "source": [
        "def file_open(ls: list):\n",
        "  data_list = []\n",
        "  for i in ls:\n",
        "    f = open(i, \"r\")\n",
        "    lines = f.readlines()\n",
        "    data_list.append(lines)\n",
        "  if len(data_list) >0:\n",
        "    print(\"file import successful\\n no. of files found:\", len(data_list))\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import files:"
      ],
      "metadata": {
        "id": "mbkYol-Zgnue"
      },
      "id": "mbkYol-Zgnue"
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = file_open([each for each in os.listdir(\".\") if each.endswith('.conllu')] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXOvqPddIIsY",
        "outputId": "497c10e8-fc91-43b2-f52a-9680084fdb21"
      },
      "id": "IXOvqPddIIsY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file import successful\n",
            " no. of files found: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c075a2-b0a0-4d08-972a-7a614e57cf3e",
      "metadata": {
        "id": "b9c075a2-b0a0-4d08-972a-7a614e57cf3e"
      },
      "outputs": [],
      "source": [
        "# extract list of sentence IDs and text in spanish:\n",
        "def sent_extractor(data:list, label:int):\n",
        "  # extract sentence id and text separately. this ensures that null values\n",
        "  # or missing data can be accounted for:\n",
        "  sent_ids = []\n",
        "  text_spn = []\n",
        "  for line in data:\n",
        "      if \"sent_id\" in line:\n",
        "          sent_id = line.split(\"=\")[1].strip()\n",
        "          sent_ids.append(sent_id)\n",
        "      elif \"text \" in line:\n",
        "      # elif \"text[spn]\" in line:\n",
        "          text = line.split(\"=\")[1].strip(\" ,\\n\")\n",
        "          text_spn.append(text)\n",
        "  print(\"number of ids and sentences in dataset:\",\n",
        "        len(sent_ids), len(text_spn))\n",
        "\n",
        "  # create dictionary with sent_id-text pairs:\n",
        "  spanish_data_dict = {}\n",
        "  for i in range(len(sent_ids)):\n",
        "    spanish_data_dict[sent_ids[i]] = str(label)+ \"  \"+ text_spn[i]+ \" .\"\n",
        "\n",
        "  return spanish_data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790a84b5-60b5-47fd-a74f-ee6605d8cb77",
      "metadata": {
        "id": "790a84b5-60b5-47fd-a74f-ee6605d8cb77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0beceb41-4c32-4736-952b-5ee1cd82726b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of ids and sentences in dataset: 76 76\n",
            "updated list of sentences:  76\n",
            "number of ids and sentences in dataset: 36 36\n",
            "updated list of sentences:  112\n"
          ]
        }
      ],
      "source": [
        "#Create dataset in the lambeq example format.\n",
        "# Take every item in each example file, and save\n",
        "# the count (ie classification label), sentence, and a period.:\n",
        "data = {}\n",
        "for count, item in enumerate(data_list):\n",
        "  data.update(sent_extractor(item, count))\n",
        "  print(\"updated list of sentences: \", len(data))\n",
        "\n",
        "#  Check length\n",
        "sents = list(data.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize sentences in sents,\n",
        "get 80%, 10% of the length of sents, in integer\n",
        "split the list into 3 sublists\n",
        "write to file"
      ],
      "metadata": {
        "id": "iQhHQOjuej6q"
      },
      "id": "iQhHQOjuej6q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2b51e3-5205-4162-9519-c653341bb9e2",
      "metadata": {
        "id": "ea2b51e3-5205-4162-9519-c653341bb9e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3a3edf-a79b-4b6d-a01b-218b9321afa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0  Estetinyol jun kiitz chawechaq .', '0  eeeh ójor laj qatinmit .', \"0  fuert alegre, alegre cuandotpe nimq'iij mayo .\", \"0  eeeh wi' jun, jun montón jb'anen tran taq pues, eeeh wi' nimq'iij .\", \"0  este, pero lamentablemente que loq'ori xan perder jun kiitz .\", \"0  jun kiitz eh tradiciones porque ójor iin xink'íych .\", \"0  eh talvez injunab' como de... aaaah ocho o nueve años, de siete a ocho, nueve años .\", \"0  wi' jun xjooj, jun xjooj, jun tradición digamos .\", \"0  este, lastima xna' desaparecer, ta' chiki'n, ta' chiki' tran taq, sach jwiich .\", \"0  y ma xaq ta iin inb'ínk, sino que k'ii ooj aj tinmit que, lastima que ta' chki'n, va .\"]\n",
            "[\"1  K'amtzáwch inb'ij Elssy Méndez, iin aj niri B'aa Kub'i .\", \"0  eeeh wi' jun, jun montón jb'anen tran taq pues, eeeh wi' nimq'iij .\", \"1  tpe taq neri laj qatinmit chi jk'ixik, tijb'an taq, tijtaq taq b'ik nimaq taq tinmit. .\", \"1  tb'e taq l chaak tk'ame' sii', tb'e taq rik'il jqaaj, ehhh chi jk'amik eh wákx, wi' jwákxaq .\", \"1  jli  tich'a'w taq kaxlan, iin cuand xintaw jli eeeh sub'laj tki'kot wánm li kristyan xink'ulaj taq. .\", \"0  eeeh de repente wi' jujun tb'now rescatar eeeh vaaa talvez tqila' chki na o ya no va .\", \"1  tijye' taq jli laj qacomunidad, jli tijye' taq aranx, limonx, y juntiir li jaa wi' jun ra chee' re q'unum .\", \"0  jalan chaq ideas o no sé aaah porque cosas, tradiciones como que tra' tri' tran valer, ta chki' tran importar rechaq .\", \"0  ri' tinb'ij iin pues vaaa de que lastima ksachVR jwiich juntiir li, aaah y ojala que algún día pues .\", '1  ojjaw l jun por ejemplo jaw temprano .']\n"
          ]
        }
      ],
      "source": [
        "# Randomize sentences\n",
        "# and save files:\n",
        "sents = list(data.values())\n",
        "print(sents[:10])\n",
        "np.random.shuffle(sents)\n",
        "print(sents[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Splits\n",
        "train_count = round(len(sents)*0.8)\n",
        "test_count = round(len(sents)*0.1)\n",
        "dev_count = len(sents) - train_count - test_count\n",
        "print(train_count, test_count, dev_count)\n",
        "\n",
        "train = sents[:train_count]\n",
        "test = sents[train_count:train_count+test_count]\n",
        "dev = sents[train_count+test_count:]"
      ],
      "metadata": {
        "id": "DYcZKtm4eSD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dba143-0c54-4300-b7a5-8688a5c7be77"
      },
      "id": "DYcZKtm4eSD7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 11 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write data files:\n",
        "def write_list_to_file(lst, prefix):\n",
        "    fname = prefix + \"_\" + lst + \".txt\"\n",
        "    with open(fname, 'w') as f:\n",
        "        for item in lst:\n",
        "            f.write(str(item) + '\\n')\n"
      ],
      "metadata": {
        "id": "A77nrVdV1FN-"
      },
      "id": "A77nrVdV1FN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_file(train, \"uspantan\")\n",
        "write_list_to_file(test, \"uspantan\")\n",
        "write_list_to_file(dev, \"uspantan\")"
      ],
      "metadata": {
        "id": "abNmweS21scs"
      },
      "id": "abNmweS21scs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}