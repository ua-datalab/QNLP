{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a99dc-98f9-4392-a4ca-8e116b52a7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install lambeq\n",
    "!pip install wandb\n",
    "!wandb login de268c256c2d4acd9085ee4e05d91706c49090d7\n",
    "!python -m spacy download es_core_news_sm\n",
    "!pip install lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea7bcfc-32da-4975-b5a8-d384dc99ef17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sent: 90it [00:00, 261.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens inthis sentence is 32 which is more than the limit prescribed. Skipping this\n",
      "no of tokens inthis sentence is 34 which is more than the limit prescribed. Skipping this\n",
      "no of tokens inthis sentence is 40 which is more than the limit prescribed. Skipping this\n",
      "no of tokens inthis sentence is 32 which is more than the limit prescribed. Skipping this\n",
      "no of tokens inthis sentence is 31 which is more than the limit prescribed. Skipping this\n",
      "no. of items processed=  85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sent: 11it [00:00, 306.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of items processed=  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sent: 11it [00:00, 300.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of items processed=  11\n",
      "85 11 11\n",
      "85 85\n",
      "85 11 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   train/loss: 0.6921   valid/loss: 0.6963   train/acc: 0.4941   valid/acc: 0.6364\n",
      "Epoch 2:   train/loss: 0.6803   valid/loss: 0.6971   train/acc: 0.5118   valid/acc: 0.4091\n",
      "Epoch 3:   train/loss: 0.6250   valid/loss: 0.6927   train/acc: 0.6471   valid/acc: 0.4545\n",
      "Epoch 4:   train/loss: 0.6091   valid/loss: 0.6839   train/acc: 0.7000   valid/acc: 0.5909\n",
      "Epoch 5:   train/loss: 0.5277   valid/loss: 0.6624   train/acc: 0.8353   valid/acc: 0.5455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING_RATE: 0.1\n",
      "Dev accuracy: 0.5454545617103577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping!\n",
      "Best model (epoch=2, step=6) saved to\n",
      "runs/Jun07_16-32-35_af20d2b07/best_model.lt\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"v4_load_uspantekan_using_spider_classical.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1YWBA_T_4phFMxts-dF8W7c1msBPhe4K1\n",
    "\"\"\"\n",
    "\n",
    "#should ideally have to do this only once per cyverse session\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.1\n",
    "SEED = 0\n",
    "DATA_BASE_FOLDER= \"data/spanish\"\n",
    "TRAIN=\"train.txt\"\n",
    "DEV=\"dev.txt\"\n",
    "TEST=\"test.txt\"\n",
    "\n",
    "import spacy\n",
    "from lambeq import SpacyTokeniser\n",
    "spanish_tokeniser = spacy.load(\"es_core_news_sm\")\n",
    "spacy_spanish_tokeniser = SpacyTokeniser()\n",
    "spacy_spanish_tokeniser.tokeniser = spanish_tokeniser\n",
    "\n",
    "import os\n",
    "from lambeq import BobcatParser\n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = float(line[0])\n",
    "            labels.append([t, 1-t])\n",
    "            sentences.append(line[1:].strip())\n",
    "    return labels, sentences\n",
    "\n",
    "\n",
    "train_labels, train_data = read_data(os.path.join(DATA_BASE_FOLDER,TRAIN))\n",
    "val_labels, val_data = read_data(os.path.join(DATA_BASE_FOLDER,DEV))\n",
    "test_labels, test_data = read_data(os.path.join(DATA_BASE_FOLDER,TEST))\n",
    "\n",
    "TESTING = int(os.environ.get('TEST_NOTEBOOKS', '0'))\n",
    "\n",
    "if TESTING:\n",
    "    train_labels, train_data = train_labels[:2], train_data[:2]\n",
    "    val_labels, val_data = val_labels[:2], val_data[:2]\n",
    "    test_labels, test_data = test_labels[:2], test_data[:2]\n",
    "    EPOCHS = 1\n",
    "\n",
    "train_data[:5]\n",
    "\n",
    "train_labels[:5]\n",
    "\n",
    "from lambeq import cups_reader\n",
    "from lambeq import spiders_reader\n",
    "from lambeq import stairs_reader\n",
    "\n",
    "# Create string diagrams based on cups reader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "parser = BobcatParser(verbose='text')\n",
    "\n",
    "def spanish_diagrams(list_sents,labels):\n",
    "    list_target = []\n",
    "    labels_target = []\n",
    "    for sent, label in tqdm(zip(list_sents, labels),desc=\"reading sent\"):\n",
    "        #using bob cat parser- note: this wasn't compatible with spider ansatz\n",
    "        tokenized = spacy_spanish_tokeniser.tokenise_sentence(sent)\n",
    "        # output_diagram = parser.sentence2diagram(tokenized, tokenised= True)\n",
    "        \n",
    "        # diag.draw()\n",
    "        # list_target.append(diag)\n",
    "\n",
    "        # tokenized_sent = spacy_spanish_tokeniser.tokenise_sentence(sent)\n",
    "        # tokenized = sent.split(\" \")\n",
    "        if len(tokenized)> 30:\n",
    "            print(f\"no of tokens inthis sentence is {len(tokenized)} which is more than the limit prescribed. Skipping this\")\n",
    "            continue\n",
    "\n",
    "        # output_diagram = spiders_reader.sentence2diagram(tokenized,tokenized=True)\n",
    "        output_diagram = cups_reader.sentence2diagram(sent)\n",
    "        # output_diagram = cups_reader.sentence2diagram(tokenized_sent, tokenised=True)\n",
    "        # output_diagram = stairs_reader.sentence2diagram(sent)\n",
    "        \n",
    "\n",
    "        list_target.append(output_diagram)\n",
    "        labels_target.append(label)\n",
    "\n",
    "    print(\"no. of items processed= \", len(list_target))\n",
    "    return list_target, labels_target\n",
    "\n",
    "train_diagrams, train_labels_v2 = spanish_diagrams(train_data,train_labels)\n",
    "val_diagrams, val_labels_v2 = spanish_diagrams(val_data,val_labels)\n",
    "test_diagrams, test_labels_v2 = spanish_diagrams(test_data,test_labels)\n",
    "\n",
    "train_labels = train_labels_v2\n",
    "val_labels = val_labels_v2\n",
    "test_labels = test_labels_v2\n",
    "\n",
    "# val_diagrams = spanish_diagrams(val_data)\n",
    "# test_diagrams = spanish_diagrams(test_data)\n",
    "\n",
    "#print and assert statements for debugging\n",
    "assert len(train_diagrams)== len(train_labels_v2)\n",
    "print(len(train_diagrams), len(test_diagrams), len(val_diagrams))\n",
    "assert len(train_diagrams)== len(train_labels)\n",
    "assert len(val_diagrams)== len(val_labels)\n",
    "assert len(test_diagrams)== len(test_labels)\n",
    "\n",
    "from lambeq.backend.tensor import Dim\n",
    "\n",
    "from lambeq import AtomicType, SpiderAnsatz\n",
    "from lambeq import MPSAnsatz\n",
    "\n",
    "# ansatz = SpiderAnsatz({AtomicType.NOUN: Dim(2),\n",
    "#                        AtomicType.SENTENCE: Dim(2),\n",
    "#                        AtomicType.PREPOSITIONAL_PHRASE: Dim(2),\n",
    "#                        })\n",
    "\n",
    "ansatz = MPSAnsatz({AtomicType.NOUN: Dim(2),\n",
    "                       AtomicType.SENTENCE: Dim(2),\n",
    "                       AtomicType.PREPOSITIONAL_PHRASE: Dim(2),\n",
    "                       }, bond_dim=3)\n",
    "\n",
    "\n",
    "train_circuits =  [ansatz(diagram) for diagram in train_diagrams]\n",
    "val_circuits =  [ansatz(diagram) for diagram in val_diagrams]\n",
    "test_circuits = [ansatz(diagram) for diagram in test_diagrams]\n",
    "\n",
    "\n",
    "\n",
    "from lambeq import PytorchModel\n",
    "\n",
    "all_circuits = train_circuits + val_circuits + test_circuits\n",
    "model = PytorchModel.from_diagrams(all_circuits)\n",
    "\n",
    "sig = torch.sigmoid\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return torch.sum(torch.eq(torch.round(sig(y_hat)), y))/len(y)/2  # half due to double-counting\n",
    "\n",
    "eval_metrics = {\"acc\": accuracy}\n",
    "\n",
    "from lambeq import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(val_circuits, val_labels, shuffle=False)\n",
    "\n",
    "print(len(train_labels), len(train_circuits))\n",
    "#print and assert statements for debugging\n",
    "print(len(train_circuits), len(val_circuits), len(test_circuits))\n",
    "assert len(train_circuits)== len(train_labels)\n",
    "assert len(val_circuits)== len(val_labels)\n",
    "assert len(test_circuits)== len(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lambeq import PytorchTrainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "trainer = PytorchTrainer(\n",
    "          model=model,\n",
    "          loss_function=torch.nn.BCEWithLogitsLoss(),\n",
    "          optimizer=torch.optim.AdamW,\n",
    "          learning_rate=LEARNING_RATE,\n",
    "          epochs=EPOCHS,\n",
    "          evaluate_functions=eval_metrics,\n",
    "          evaluate_on_train=True,\n",
    "          verbose='text',\n",
    "          \n",
    "          seed=SEED)\n",
    "\n",
    "trainer.fit(train_dataset, val_dataset, eval_interval=1, log_interval=1,early_stopping_criterion='acc',early_stopping_interval=3)\n",
    "\n",
    "\n",
    "# print dev accuracy\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# fig1, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharey='row', figsize=(10, 6))\n",
    "\n",
    "# ax_tl.set_title('Training set')\n",
    "# ax_tr.set_title('Development set')\n",
    "# ax_bl.set_xlabel('Epochs')\n",
    "# ax_br.set_xlabel('Epochs')\n",
    "# ax_bl.set_ylabel('Accuracy')\n",
    "# ax_tl.set_ylabel('Loss')\n",
    "\n",
    "# colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "# range_ = np.arange(1, trainer.epochs+1)\n",
    "# ax_tl.plot(range_, trainer.train_epoch_costs, color=next(colours))\n",
    "# ax_bl.plot(range_, trainer.train_eval_results['acc'], color=next(colours))\n",
    "# ax_tr.plot(range_, trainer.val_costs, color=next(colours))\n",
    "# ax_br.plot(range_, trainer.val_eval_results['acc'], color=next(colours))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b404a3f-e004-4ac5-b658-496ea768145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING_RATE: 0.1\n",
      "Dev accuracy: 0.5454545617103577\n"
     ]
    }
   ],
   "source": [
    "# print dev accuracy\n",
    "dev_acc = accuracy(model(val_circuits), torch.tensor(val_labels))\n",
    "print('LEARNING_RATE:', LEARNING_RATE)\n",
    "print('Dev accuracy:', dev_acc.item())\n",
    "\n",
    "\n",
    "# print test accuracy- use this ony once\n",
    "# test_acc = accuracy(model(test_circuits), torch.tensor(test_labels))\n",
    "# print('Test accuracy:', test_acc.item())\n",
    "# wandb.log({\"test_accuracy\":test_acc.item()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
