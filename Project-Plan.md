# References and links:
* link to mithun experiments/tuning with uspantekan is kept [here](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing)
* Link to github started by Robert Henderson: [here](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp&sa=D&source=editors&ust=1717607867014854&usg=AOvVaw3ji0W3TH7OhJaizgZHp14m)
	* QNLP dataset: [https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data](https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data)
	* QNLP code repo: [https://github.com/ua-datalab/QNLP/blob/main](https://github.com/ua-datalab/QNLP/blob/main)
* Link to white paper: [https://www.overleaf.com/4483532232tcfnfdrrcbdc#12a1b4](https://www.google.com/url?q=https://www.overleaf.com/4483532232tcfnfdrrcbdc%2312a1b4&sa=D&source=editors&ust=1717607867015283&usg=AOvVaw0VwgWn_tu2jNMuTmaj2PDL)
* All data (e.g. spanish only files) is stored in a [gdrive folder here](https://www.google.com/url?q=https://drive.google.com/drive/folders/1m4nFZwsUcZ2DQzN3nYaK0_oKJXGhV575?usp%3Ddrive_link&sa=D&source=editors&ust=1717607867015673&usg=AOvVaw32Cbwsxm70wOGxbbRLFbb0)
	- Uspantekan data: [https://drive.google.com/drive/folders/1CtMhTf-v0nSUSaTJVelILkDMrLfF1U5Y?usp=share_link](https://drive.google.com/drive/folders/1CtMhTf-v0nSUSaTJVelILkDMrLfF1U5Y?usp=share_link)
 	- Spanish data: [https://drive.google.com/drive/folders/1SThJ6tyUAzvfVSFo6w_VyB4HPt381jp1?usp=share_link](https://drive.google.com/drive/folders/1SThJ6tyUAzvfVSFo6w_VyB4HPt381jp1?usp=share_link) 
* Jira Link: [https://cyverse.atlassian.net/jira/software/projects/QNLP/boards/27](https://www.google.com/url?q=https://cyverse.atlassian.net/jira/software/projects/QNLP/boards/27&sa=D&source=editors&ust=1717607867016357&usg=AOvVaw2fccm9pIgF5Yw5sAb26eH0)     
* [Miro Whiteboard](https://miro.com/app/board/uXjVKVPCIK4=/?share_link_id=77584526552) 
* Papers:
	* 1998 Lambeks paper on math===language: [here](https://www.google.com/url?q=https://drive.google.com/file/d/1BWhs5zOoA2n7y8aUnKoamfift0t9Xdhu/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867016692&usg=AOvVaw3z0FGavJsHiiA0aRD5yFLn)
	* 2020 bob coecke [QNLP](https://www.google.com/url?q=https://drive.google.com/file/d/15hXA_ecFN31JJdt9E8POdUFT1mlcwssv/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867017007&usg=AOvVaw2jRw8msgoQEVE_z5vZxQCa)
	* Type grammar revisited: [https://link.springer.com/chapter/10.1007/3-540-48975-4\_1](https://www.google.com/url?q=https://link.springer.com/chapter/10.1007/3-540-48975-4_1&sa=D&source=editors&ust=1717607867017296&usg=AOvVaw0T99YvALpGGqp50dAnYxz9)
	* Khatri et al. thesis: [https://drive.google.com/file/d/141UTEmduZoFhq0-d1peWLGdUhMLpNlho/view?usp=share_link](https://drive.google.com/file/d/141UTEmduZoFhq0-d1peWLGdUhMLpNlho/view?usp=share_link)
* Colab notebooks:
	- Data pipeline: [https://colab.research.google.com/drive/1YwdVkFZRt30QPuUwQ-y9W1vSnYlkS656?usp=sharing](https://www.google.com/url?q=https://colab.research.google.com/drive/1YwdVkFZRt30QPuUwQ-y9W1vSnYlkS656?usp%3Dsharing&sa=D&source=editors&ust=1717607867017671&usg=AOvVaw3sePnYQ_2mwLcqo1YYvu9Y)
	- Lambeq for Spanish, run with Spider parser: [https://drive.google.com/file/d/1wTo8rAObpuLu65DyFo1D0gE5kKjUtzBf/view?usp=sharing](https://www.google.com/url?q=https://drive.google.com/file/d/1wTo8rAObpuLu65DyFo1D0gE5kKjUtzBf/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867017990&usg=AOvVaw1oMypNSQtjg_K-olMfxRnv)
 	- Khatri et al, for Spanish: [https://github.com/ua-datalab/QNLP/blob/megh_dev/OOV_MRPC_paraphrase_task.ipynb](https://github.com/ua-datalab/QNLP/blob/megh_dev/OOV_MRPC_paraphrase_task.ipynb)
  	- Khatri et al, for Uspantekan: [https://github.com/ua-datalab/QNLP/blob/mithun_dev/v2_khatri_thesis_version_which_gave1_mnli_100_runs_end_to_end.ipynb](https://github.com/ua-datalab/QNLP/blob/mithun_dev/v2_khatri_thesis_version_which_gave1_mnli_100_runs_end_to_end.ipynb)

# Working Group Best Practices
- Save all data in the private repository, to prevent leaks: [https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data](https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data)
- Save code to public repository, so it can be opened on Cyverse and run on colab: [https://github.com/ua-datalab/QNLP/blob/main/](https://github.com/ua-datalab/QNLP/blob/main/OOV_MRPC_paraphrase_task.ipynb)
- Cyverse has resource allocations- so all big training done there. Example: 


# Meeting Notes
## September 23th 2024
- Created a flowchart explaining Khatri et al.'s code, to understand each process in the model used
	- Why is there is NN model (MLP) between the input and QNLP model?
		- Section 62- NN is learning a map from embeddings of vocalbulary in training set to QLP model's trained parameter assignment. This helps "generalise unseen vocalbulary".
	 - Does the Uspantekan model without embeddings require an NN at all? No, because there are no embeddings to map.  
	 - What are the input and outputs to the QNLP model with embeddings?
  - Steps for training:
  - For every word in training vocabulary:
  	1. Thread 1:
   		1. Get embeddings from Fastest
        	2. Generate vector representations
         	3. This is a **pre-trained model** 
	2. Thread 2:
  		1. Initialize some non-zero weights for the QNLP model
    		2. In parallel, run the training dataset through lambeq ansatz, and obtain circuits at the sentence level.
    		3. Use these circuits as training data and **train the model** to get weights
      		4. Obtain word-level representations from the model weights. TODO: how do we generate word-levels representations from sentence-level trained weights? 
	3. Bringing it together:
   		1. Create a dictionary by mapping each word in the vocabulary with weights from the QNLP model (which is also a vector representation). DON'T KNOW HOW
     		2. Create list of lists mapping each word to a vector representation of its fasstext ebedding. TODO: check how this is done.
       		3. **Train a model** to find a relationship between word-level vector representation from QNLP model weights, and the embeddings from fasttext
         	4. **Train another model**: TODO- find out why
        4. Testing the final model 



## September 18th 2024
- Discussion of code v7
	- `train.preds()` did not work: Pytorch tensors include the loss values plus the computational graphs, so there would be an issue running them with numpy
 	- Solution: `torch.detach()` which creates a copy of the tensor, without the graph
  - Substitute hard-coded loss function with `torch.nn.loss`
  - Fixed error that combined train, test and dev for generating training data
  - what is `embedding_model`- fasttext
  - ToDo: draw a flow diagram to understand what `qnlp_model` is doing
  - DNN model: training between word and its embedding
  - QNLP model: angle and word
  - ToDo: find difference between symbol and word
  - ToDo Mithun: refactor code to ensure all functions are in the correct order, based on Khatri et al.

## September 16th 2024
- Tech Launch Arizona funding
	- AI + Quantum applications
- QNLP applications in Megh's dissertation
	- LLMs are trained on next sentence prediction, entrainment looks for people sharing linguistic features
- Discussion on QNLP applications:
	- RAG also a good fit for indegenous languages? May be a low-lying fruit, even if it runs into data issues
- Sprint: debugging v7 code
	- What does maxparams do? Spanish Fasttext model provides 300 parameters. So, the number is hard-coded into the code
 	- OOV words were an issue, changed max_params to fix this
  	- Setting up loss code and accuracy code
- Pytorch vs Keras
	- Using pytorch for calculating angles in place of numpy
 	- But if keras is being used   

## September 11th 2024
- Code sprint and logging and documenting bugs
	- Spanish embeddings model:
 		- added v7 to `megh_dev` and adding documentation.
   		- See: [code compare for more](https://github.com/ua-datalab/QNLP/commit/3472a3addd948076677e83c70dfd36892b38c41a) 
		- 1 AI model + MLP for mapping spanish embeddings with spanish text
  		- Initial model fixed by switching from numpy to pytorch

## September 9th 2024
- Uspantek + spanish
	- run more experiments to confirm the results 
   	- cons: Mithun wants to read more before turning knobs.
- write paper:
	- reproduce results again?
 	- confirm with robert, 
 		- we can use the uspantek data.
 		- remind him to connect with his collaborator's phd student.
- upcoming deadlines
	- NLP
		- COLING: sep 17th 2024  
		- ICLR: 2nd oct 2024
		- NAACL: Oct 16th 2024.
	- Computational linguistics
		- SCIL- december 2024
		- [LSA](https://web.cvent.com/event/40d9411e-b965-4659-b9c3-63046eeed3d4/summary)
- update: we decided to start writing paper- kept [here](https://www.overleaf.com/4483532232tcfnfdrrcbdc#12a1b4). AIm is for ICLR 2nd october. But more importantly, it is an exercise to capture all the thoughts fresh in our head
- Paper to-dos
	- Write introduction
	- get a lit review for:
 		- QNLP,
   		- Uspanteko,
		- low-resource language LLM research:
  			- https://arxiv.org/pdf/2406.18895
     		- https://aclanthology.org/2024.americasnlp-1.24.pdf
        	- https://arxiv.org/pdf/2404.18286  
  	- Results and methods
  		- Baseline models for Spanish, Uspantekan
  		-  Future: Fasttext embeddings for Spanish model
- Current issues:
	- Embeddings model has a lot of bugs
 - todo
 	- wed meeting,
  		- Mithun will try to push through code of spanish + embeddings, with the aim of: lets have closure/have 3 systems working
  		- Mithun: find more AI related NSF - QNLP for indigenous languages
   	- 	 

## September 5th 2024: Meeting with Robert
- Dataset has other topics related to education and dance: like "teaching Uspantekan", and other forms of dancing
	- We have more data! 
- [Link to Slides](https://docs.google.com/presentation/d/1jw9_b55BC4HOMmtOaqbXjDkOR8nn9_Zg8xLjK86KG8w/edit)
- Robert's update:
	- Currently: 1800 sentences, 12k tokens, with dependency parsing
	- 10k tokens for different discourse types
	- Plus 5 other languages with spanish dictionary  
	- QNLP has its own parser- but throws out a lot of sentences which it can't parse
	- Super helpful update about pre-parsed sentences!
- Target: which NSF project should we consider?
      - "Dynamic language infrastructure and Language Documentation" grant currently funds Robert, along with CISA (proposed by NSF).
	- Maybe a good option
	- Target date: Feb 18th 2025 [link](https://new.nsf.gov/funding/opportunities/nsf-dynamic-language-infrastructure-neh)
	- Better than AI grants, dominated by LLM
	- NSF doesn't like to fund the same grant twice- so keep both projects meaningfully different!
		- Check if QNLP can be imagined as an extension or addition to an NSF grant
		- Extension with more money- check if it's a thing for linguistics-focussed NSF, Robert could also get more funding. 
- Moving away from dataset creation grant, to a new theoretical framework (QNLP)
	- Work on more languages, so focus is on low-resource languages generally
 	- Another important focus: focussing on why this technology is a good fit for a particular use-case (low resource languages), and target tasks that human annotators are really struggling with
- Ex. automating: morphological analysis, parsing, POS taggging, spell-checkers, word search 
- What is the ask in terms for funding?
	- Personel: funding for Mithun, an RA/postdoc
	- Robert's experience: his share is 135k, most of it goes to the collaborator's funding, plus annotators
- Other next steps: check out of the box QNLP can help with tagging
   - Compare how the current AI automatic tagger trained by Robert's collaborator does compared to this
   - Mithun: read Robert's grant

## September 4th 2024
- v4 code running end-to-end. Why?
	- Code was stuck before `fit()`.
 	- Code switched from `numpy` to `pytorch`, needed to switch from arrays to tensors
  	- Ansatz- issues
  	- Parser language settings- when the parser encodes
  		- Embeddings from fasttext was different from what `lambeq` expected:
  	 		- Parser from `lambeq` adds underscores and to each entry, which is missing in the fasttext embeddings. Needed cleanup to prevent mismatch.
		- Khatri et al. expects a double underscore at the end of each entry
	- Padding for symmetrical arrays that `numpy` needed. So, switched to Pytorch models. Hopefully, quantum models will also not have this issue.
 	- Expected issue, as no one has used spanish embeddings.
  - Big picture- end-to-end model for Uspantekan and Spanish
- Notes for meeting with Robert
-  Current baselines: with about 100 sentences as input
	- Spanish:
 		- without embeddings, with 100 sentences, classification accuracy is 71%. Very tunable
 		- Next plan: see what classification accuracy we get with embeddings
   	- Uspantekan
   		- Uspantekan data, no embeddings
   	 	- Classification accuracy: 72%
- Next steps:
	- see what classification accuracy we get with Spanish embeddings, to see if embeddings can improve scores. This will help us rely on non-English text
 	- Tuning
  	- Get F1 scores to assess all quadrants of the testing
  	- Assess with quantum computing is able to get us closer to underspecificity.

## August 28th 2024
- Why Spider? It works, gives results- use it for Spanish, as well as uspantekan
	- Use for both cups and ansatz
	-  
- Model
	- Use BCE loss- original code coded it, instead of calling it
 - To Do
 	- v6 code- lambeq and pytorch have version issues
  	- Do `lightning` and `lambeq` work together?
   - try with v6 code and pytorch
    	-getting the symbol doesnt have size issue in laptop. Sounds like a version mismatch between lambeq and pytorch
    	- try on colab
      	- update- gets the same error on colab
       	- try v4 on colab.-update: works fine. 
        - also try v4 on laptop
        - if no error related to pytorch in trainer.fit:
        - 	update@august31st-OOV error, . fixed by taking .fit() out of wandb sweep.v4 ran end to end for both spanish and uspantekan- atleast till trainer.fit since v4 didnt have the second ML model that khatri uses
        - 	find why v6 is not running.
        - 	update@august31st-v6 still giving OOV error in first model
        - 	remember: goal here is to get the spanish to work end to end with spanish embeddings. t
        - 	then we willt hink about aligning with uspantekan translations for OOV in uspantekan
        - 	
- 	else:
        - switch to Quantum Model- using actual quantum computer. If we are fighting stupid infrastructure and dll issues might as well do it for quantum model, not stupid pytorch or numpy models.
  	
## August 26th 2024
1. try with NUMPy model and square matrix issue
	-  try with making all sentences padded with . -- failed -bob cat parser, automatically removed . 
	-  try with same sentence- works fine./no matrix/array difference issue
	-  sentence level: 
	-  diagram level: ignored
	-  circuit level: tried adding dummy circuit. i.e say XX gates back to back ==1 but became a pain since they wanted it in Rx Ry gate. 
		- why was this not a problem earlier
   		- why 10?
- why did this not happen in the english version- or even uspantekan version?- our own code?
	- in khatri tehsiswas he terminating english sentence- go back and look at his original code- answer: no, he is also doing same maxlength <=
- what he is doing with maxlen.- picks sentences less than maxlength. 
2. what are the potential solutions
	- without quantum level change
	- try with period.--failed
	- try with filler words. uhm--failed
	- tokenizer spanish
	- how is our own english/uspantekan code different than the spanish one. Are we using a different spider?
	- spacy tokenizer
3. update. we decided to do this comparison first. i.e compare between v4 (the code which worked end to end for uspantekan) kept [here](https://github.com/ua-datalab/QNLP/blob/mithun_dev/v4_load_uspantekan_using_spider_classical.py) and v6(the code which is not working for spanish) [here] (https://github.com/ua-datalab/QNLP/blob/mithun_dev/v6_qnlp_uspantekan_experiments.py)
	- how is khatri's code different than the spanish one. Are we using a different spider?
	- with quantum level change
4. Replace with quantum model/simulation?
5. once we have end to end system running, to improve accuracy, add a spanish tokenizer expliciity

## August 21st 2024
- Main bug- Lambeq's classical example uses Numpy, which is set up to require square matricies as input.
	- Khatri et al. uses the first 10 words in the sentence, discards the rest.
 	- Code does not run when sentences have >10 words
- Potential Solutions
	- try padding sentences with special characters or filler words?
  		- This did not workwith special characters, which got filtered out
  	- Filler words like "um"
  	- Choose n_words >10, based on the data?
  		- ToDo Megh: work on this
 - Mithun contacted Robert Henderson, requested meeting  

## August 9th 2024
- Quick discussion on the difference between raw embeddings and other options
- ToDo Tuesday: create materials for student presentation, and progress in the QNLP project
- Debugging the code to fix fasstext issues
	- The print statements are printing circuits, making it hard to see which words are the issue
 	- How are OOV words identified and their no. calculated? Is Fasttext being implemented correctly?
	-  not the main issue- embeddings are being used correctly    
  	- Is the code using the fasttext embeddings at all? or the spacy spanish parser?
  	- Updates to code- added a try-except chunk: when a word is really OOV, the code will stop and print details for us
  	- Examine why training loop is utilizing only 14 out of 90 sentences, why is bobcat parser not working with the rest?
  	- Embeddings not being passed in the right format- hence a shape error

## July 30th 2024
- General discussion for wrapping up summer responsibilities- and plan going forward
- Close reading of section 6.1-6.3
- skip gram model vs context-aware methods
- Mapping fasttext embeddings and the given input

## July 25th 2024
- Looked at the Khatri et al. code for Spanish, and worked on code fix: https://github.com/ua-datalab/QNLP/tree/megh_dev
	-  Most of the word are OOV, so they can't be put into grammatical categories
	-   ToDo: Assess what khatri et al. does with OOV cases (section 6)
	-   ToDo: run code on VSCode
- General Discussion:
	-  How does Quantum trainer compare to NN?
 		- Feed forward- we look at the loss between the original and predicted value, does back propagation, until the right combination of weights provides us useable prediction
  		- Instead of neurons, we use quantum circuits
    		- Instead of parameters for gates, we have a different system
      		- Objective- minimize loss, and get optimal assignment of parameters, best combo of weights, to find mapping between input setence and label. "Why does a group pf words belong to x category, not y"?
        	- What is loss value?
        		- Classical: diff between gold and predicted, we backprop
          		-  QNLP- Instead of weights, think of angles. machine's job is to find optimal "angles"  
  		- Thus, the trainer for QC is the same as NN- both have similar black boxes
    			- Very hard to explain the difference between two sentences with the same syntax, but different meanings.
      		- Different words will have different angles- so we can explain semantic differences in syntactically identical sentences
		- The code should look the same for deep and quantum trainer
- What is "OOV", from a coding perspective?
	- out of vocabulary words are assigned the same weights, which is not an accurate way to proceed
	- Fast text, Byte Pair Encoding: ways to solve this problem by using embeddings (n-gram models) to assign different weights to different OOV words
 		- Our code needs to learn some kind of mapping between ngrams and angles  
	- In our code, nearly all words are assigned OOV labels

## June 30th 2024
- Discussion of Khatri et al., section 6
	- Continuous bag of words model- teaching a machine a simple pattern matching method. When X word exists, works Y, Z are also likely to occur.
 	- "meaning is context", not "context is meaning".
  	- Word embeddings have a similar understanding of mental lexicon as expeiments on lexical semantics?
  	- Fast text- improving OOV issues by either working with n-grams (thus, meaning agnostic)
  	- Mithun's idea- use GPT embeddings which may take care of OOV words by assessign words in a network of related words, rather than related phones.
  	- 6.2- words belonging to one topic or category will be seen together. Two models are used, a general embedding, as well as a perceptron trained on task-specific vocabulary
  	- Implementation of the baseline models- deeper models performed better than surface models
  - ToDo- look at 6.0-6.4 again, and come back with notes. 

## June 25th 2024
- Pivot to actually determining what the LLM baseline classification accuracy is for our dataset, os that we know what the quantum approach needs to beat.
- ToDo Megh:
	- Move Mithun's single python file to our repo
 	- Edit to load Uspantekan data 
	- Run the LLM code, an untrained DistilBERT and RoBERTa using Mithun's codebase
	- Report bugs and/or result.
- Mithun tries to load embeddings + quantum
  
## June 21st 2024
- Updates to khatri et al code:
	- Current work on Spanish data, using khatri et. al.: [https://github.com/ua-datalab/QNLP/tree/megh_dev](https://github.com/ua-datalab/QNLP/tree/megh_dev)
 	- Mithun shared his updated code for khatri et. al., that works on Uspantekan:  https://github.com/ua-datalab/QNLP/blob/mithun_dev/v4_load_uspantekan_using_spider_classical.py
  - 
  - Overhauled code to fit our classification task that has only one feature vector, as opposed to two. (that is because khatri code was designed for NLI kind of tasks, which expects a pair like hypotehsis and premise)+ `lambeq` libraries and modules needed to be replaced due to depreciation.

## June 7th 2024

* How to access shared analyses on Cyverse- Mithus shares his setup so we have access to GPUs
* Go to [https://de.cyverse.org/analyses/](https://de.cyverse.org/analyses/) and switch the dropdown menu to "shared with me"
* Choose the relevant analysis.
* Setup and tech issues
	* Get permission to run analysis with a GPU on cyverse. Talk to Michele about permissions
 	* Set CPU to 8GB 

## June 5th 2024
* [results location]([url](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing))
1. Can qnlp + uspantekan- straight out of the box give a good classification accuracy?
	* update: NO
	* Rather after tuning max accuracy on dev was 72%...which was given by cups and stairs model
1. Options
	1. path 1:
		1. go back to  spanish- and load with embeddings.
	2. path 2: try with discocat/bobcar parser + uspantekan.. last time we tried, got errors.
* Updated Jira with new tasks and updates
* Megh's updates:
	* Set up repo for code, as well as a folder with dataset in rhenderson's repo:
 	* QNLP dtaset: https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data
 	 * QNLP code repo: https://github.com/ua-datalab/QNLP/blob/main/Project-Plan.md	 
* Todo for Megh
	* Run the code for Spanish- we will need embeddings
	* Use fasttex- Used by GPT for building ngrams and aligned work vectors. 
	* Thesis code from Mithun’s contact (Khatri et al.)
		* Run this code directly and see what happens
* Examined issues received outlook calendar nvotes

## May 30, 2024 :
* Jira setup for project
* Classical model pipeline with Spanish up and running
	- Major update- removed all sentences with >32 tokens, and the Spanish model was able to run with lampeq
	- Training accuracy is good, but loss is too high
	- Needs finetuning
* What is the language model in th classical case doing?
	- Spanish SpaCy model is using the tokenizer to be able to use word-level knowledge.
	- Then, bobcat uses the sentence2diagram method on the result.

* New proposal- to run Spider directly on the uspantekan sentences
* ToDo- move notes to Github, create notes.md and keep all data on Github
* Jira for project setup-
	- Todo- assess the space and make sure Megh knows her tasks.
	- Check out QNLP52
* Paper writing
	* Pivot from a whitepaper to a one-pager
		* Motivation, contribution thought

## May 7th, 2024:
* Classical case:
	- Filename non-ASCII issue resolved with rename   and train, test, dev splits saved
	- Classical case-  [ran into error while running sentence2diagram](https://www.google.com/url?q=https://colab.research.google.com/drive/12kNxLNX162hGznIYenBSqLJbflmFaE1y?usp%3Dsharing&sa=D&source=editors&ust=1717607867019672&usg=AOvVaw1SAvjipfXEAOkwHKcnRCgQ)
	- Check classical case with Spanish, assign spanish SpaCY language model to pipeline if needed
	- ToDo for Mithun
	- Fix classical trainer, section “Training the model”, cell 67
* Whitepaper updates: Mithun shared his current working document

## April 19th, 2024:
* Issue with non-ASCII characters in filename persists- can’t read from file
* Pipeline for reading sentences ready- need to work on reading files in with non-ASCII filenames
* Todo:
	- Mithun- set up an overleaf for a paper draft
	- Mithun- find out how to run QUANTUM CASE on Cyverse
* Data pipeline todo \[Megh\]:
	- randomize sentences, and create test, val, train splits
	- Convert the sentences to a dataset in the format: Label\[Tab\]Text\[Tab\].
		+  0 for bailar, 1 for educacion
* ToDo Mithun: create list of file names, and convert them to ASCII

## April 11th, 2024:

* Updates
	- Classical case set up on Cyverse
	- Data cleaning pipeline setup
* ToDo: upload code to Colab \[DONE\]
* Todo:
	- Mithun- set up an overleaf for a paper draft
	- Mithun- find out how to run QUANTUM CASE on Cyverse
	- Megh- find a way to run data pipeline on Cyverse \[DONE\]
	- Megh- complete data cleanup pipeline \[DONE\]
* Proof of concept:
	- Data- done
	- Data cleanup pipeline: done
	- Code- quantum case in process
* Data pipeline todo: \[DONE\]
	- randomize sentences, and create test, val, train splits
	- Convert the sentences to a dataset in the format: Label\[Tab\]Text\[Tab\].
		+  0 for bailar, 1 for educacion
	- ToDo Mithun: create list of file names, and convert them to ASCII
* QNLP pipeline setup
	- ToDo Megh: Run classical case on Cyverse with Spanish data

## April 2nd, 2024:

1. We went through learning
	1. Bobcat parser
	2. Convert text to diagrams
	3. Convert diagrams to circuits
	4. Rewriter+ reduce cups
	5. Go through full fledged training example of classification of IT or ood
2. Todo \[Megh\] for next week:
	1. Set up and Try out lambeq classification [task](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-classical.html&sa=D&source=editors&ust=1717607867023254&usg=AOvVaw3Se6n9TAlMX5oyEHCO2C1Z)  on Cyverse
	2. With spanish text from the Robert henderson’s data
		1. Pick 2 classes (i.e file names in the data directory)
			1. dancing/bailes
			2. [Education](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp/blob/main/data/UD/Acontecimientos_sobrenaturales.conllu&sa=D&source=editors&ust=1717607867023895&usg=AOvVaw3gzoN1EoQiznA9p15PbxWz)
			3. Try to recreate the same ML pipeline shown in the lambeq task above.
				1. Using spacy spanish tokenizer.
3. Concrete steps:
	1. Download code from the Lambeq tutorial’s   [Quantum](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html&sa=D&source=editors&ust=1717607867024391&usg=AOvVaw0hSG13wlkmHZP5akrEMoPD)  case
	2. Replace [training](https://www.google.com/url?q=https://github.com/CQCL/lambeq/blob/main/docs/examples/datasets/rp_train_data.txt&sa=D&source=editors&ust=1717607867024688&usg=AOvVaw0C4Tf2Ane5m0bQGPVI7Mj4)  data from relative clauses to the text classification task (see ‘Classical Case’ [https://cqcl.github.io/lambeq/tutorials/trainer-classical.html](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-classical.html&sa=D&source=editors&ust=1717607867024954&usg=AOvVaw0iw7KHxbXYMsbUCXhl0ft6)
	3. Run the code and assess performance.
	
	4. Lambeq- removed “glue” words, or ones that don’t add to the semantics of a sentence. these correspond to the stop words of a language
	5. Colab notebook: [https://colab.research.google.com/drive/1krT2ibzrfLxin6VT5-nyXWr8\_HGEHNVF?usp=sharing](https://www.google.com/url?q=https://colab.research.google.com/drive/1krT2ibzrfLxin6VT5-nyXWr8_HGEHNVF?usp%3Dsharing&sa=D&source=editors&ust=1717607867025382&usg=AOvVaw3m2tOfqrp4UBXV1cY_-H13)
	6. research ansatz
		1. Advantage- it is able to better capture the richness of meaning
	7. Current status- spanish spacy tokenizer is able to tokenize correctly for Spanish. However, QBIT assignment is not working.
		1. How do we initialise the qbits?

8. Pipeline for the task:
	1. Use the same task as tutorial: [https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html&sa=D&source=editors&ust=1717607867025982&usg=AOvVaw2jfGhkUkfrlNbkzWkBIR2t)
	2. data- 100 spanish sentences from the uspantecan corpus
	3. ToDo- plan a classification task for uspantecan
	4. Robert Henderson’s repo: [https://github.com/bkeej/usp\_qnlp](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp&sa=D&source=editors&ust=1717607867026388&usg=AOvVaw0W2XkIkcQV3xFiThDTApyS)
	5. We select 2 data files, with different topics
	6. We create a classification task for differentiating between sentences under each topic

## Mar 19th, 2024

* What is [DisCoCat](https://www.google.com/url?q=https://cqcl.github.io/lambeq/glossary.html%23term-DisCoCat&sa=D&source=editors&ust=1717607867026871&usg=AOvVaw1JT_SF8YpM08KtOmjl2vs9) ?
	- Discrete mathematical category
* How do we choose a parser?
	- BobCat parser is the most powerful, so that’s the one bing used.
* Spider reader- we are not trying to build a bag of worlds model. We want to keep the grammar
	- It is faithful to lambeq
* Fix error with unknown word handling
* Monoidal structure-
* Start code for Spanish
	- Can bobcat work with Spanish?
	- how will PoS
* Step 3 Parameterization-
* Import Robert’s data
* ToDo- how to collaborate on a jupyter notebook cyverse
* ToDo- compile their code locally- edit SpacyTokeniser to change language from English to Spanish. Mithun is discussing this with the lambeq team
* ToDo- set up the dataset from Github to Cyverse, and use spanish translations.

# Mar 12th, 2024

* 1998 Lambeks paper on math===language: [here](https://www.google.com/url?q=https://drive.google.com/file/d/1BWhs5zOoA2n7y8aUnKoamfift0t9Xdhu/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867028080&usg=AOvVaw1KcGOV2dbqQGcyH1m-yBwy)
* 2020 bob coecke [QNLP](https://www.google.com/url?q=https://drive.google.com/file/d/15hXA_ecFN31JJdt9E8POdUFT1mlcwssv/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867028363&usg=AOvVaw0YfZ4zJY2KeDRyG4ZxWA1c)
* Type grammar revisited: [https://link.springer.com/chapter/10.1007/3-540-48975-4\_1](https://www.google.com/url?q=https://link.springer.com/chapter/10.1007/3-540-48975-4_1&sa=D&source=editors&ust=1717607867028593&usg=AOvVaw3TiubEsWFejcv65hog7NXV)
* [https://cqcl.github.io/lambeq/](https://www.google.com/url?q=https://cqcl.github.io/lambeq/&sa=D&source=editors&ust=1717607867028793&usg=AOvVaw2RpORwY67SCqRm2u84fRef)
* Neural networks use stochastic gradient descent- a top down approach. Language needs a bottom up approach
* Category theory hierarchy- everything is a category.
* Lambek- The Mathematics of Sentence Structure
	- Language is a bag of things, with 3 things- noun, sentence, NP. Anything can be created from these three
* Type Grammar revisited (Lambek 1999)
	- groups and proto groups- when an operation is done on members of a group, it remains in the same group?
* Combinatory Categorical Grammar
* Qubit- every time a decision is made, multiple options are collapsed into one. Until a decision is made, all possibilities are true. A qubit hangs out in infinite space, until it is acted upon by an operator.
* Quantum model for NLP Coecke
	- one grammatical category acting on a word, to move it one way or another
	- Dependency parsing looks like matrix multiplication
* Bag of words
	- sentences split into smaller meaning carrying chunks (words), which can be interchangeably combined in different ways
	- However- word combination is governed by semantic relationships between words
* Lambeq- pip install lambeq  to install
* If we know the minima of the gradient descent- can we build language up from it?
* TODO- install lambeq  and feed it a sentence \[done on Cyverse\]
* Run end to end- work on it like a tutorial
* Think- tokenizer available for English, Spanish, but not other languages. How do we work without one?
	- Run this on Spanish first
	- Think of a problem
* Jupyter notebook stored at: /data-store/iplant/home/mkrishnaswamy/qnlp

# Project plan 
Goal: we want to show Robert a proof of concept that QNLP can work with uspantekan- limited resources- and still give good accuracy
1. Can qnlp + uspantekan- straight out of the box give a good classification accuracy- if yes:
	1. Path 1 bottom up:  
		1. Pick one thread. Eg. spider
			1. Trying on spanish
			2. Find embedding spanish
				1. Split by space- what is accuracy
				2. Try splitting - Spanish tokenizer- did accuracy improve
			3. Align with embedding uspantekan
			4. Run classification task again on uspantekan
	2. Path 2
		1. Qn- why don‘t we straight Run classification task again on uspantekan -with spider
			1. No parser which can break tokens faithfully
			2. No embeddings directly for uspantekan
			3. How much is bare bones accuracy?
			4. With tuning how much can you get it upto?
			5. If they both fail,
			6. Then yes, we can think of bringing in spanish embeddings.
			7. Todo
				1. Train dev of uspantekan
				2. Modifying
2. update: june 5th 2024
	1. Path 2: Ran experiment. [here](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing) are the results of trying Uspantekan with spider parser, bobcat parser, and using pytorch trainer. Rather after tuning max accuracy on dev was 72%...which was given by cups and stairs model -so we have decided to move on.
	2. Options to explore next
		1. path 1: go back to  spanish- and load with embeddings.
		2. path 2: try with discocat/bobcar parser + uspantekan.. last time we tried, got errors...
3. update: june 25th 2024:
	- still working on QNLP +uspantekan + embedddings. reevaluated goal and pivoted because of the question: what baseline
	- are we trying to beat. Decided we will do the baseline first on LLMs
      
# General correspondence:
- why did we decide to go with spanish first and not Uspanthekan?
	- Lambeq pipeline seems to have a language model requirements and needs embeddings. We have some for Spanish, none for Uspantekan
	- We have direct Uspanteqan-Spanish translations, but not English-Uspanteqan. Which means that if things fail, we have no way to examine what happened if we used an English model.
- How many hours can Megh work on QNLP?
	- This semester: 5hrs, as workshop is also a responsibility
 	- Two 10hr projects may derail dissertation
- Spring 2025: 12/8 split will work, as fewer hours needed for creating content    
