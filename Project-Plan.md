# References and links:
* link to mithun experiments/tuning with uspantekan is kept [here](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing)
* Link to github started by Robert Henderson: [here](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp&sa=D&source=editors&ust=1717607867014854&usg=AOvVaw3ji0W3TH7OhJaizgZHp14m)
	* QNLP dataset: [https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data](https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data)
	* QNLP code repo: [https://github.com/ua-datalab/QNLP/blob/main](https://github.com/ua-datalab/QNLP/blob/main)
* Link to white paper: [https://www.overleaf.com/4483532232tcfnfdrrcbdc#12a1b4](https://www.google.com/url?q=https://www.overleaf.com/4483532232tcfnfdrrcbdc%2312a1b4&sa=D&source=editors&ust=1717607867015283&usg=AOvVaw0VwgWn_tu2jNMuTmaj2PDL)
* All data (e.g. spanish only files) is stored in a [gdrive folder here](https://www.google.com/url?q=https://drive.google.com/drive/folders/1m4nFZwsUcZ2DQzN3nYaK0_oKJXGhV575?usp%3Ddrive_link&sa=D&source=editors&ust=1717607867015673&usg=AOvVaw32Cbwsxm70wOGxbbRLFbb0)
	- Uspantekan data: [https://drive.google.com/drive/folders/1CtMhTf-v0nSUSaTJVelILkDMrLfF1U5Y?usp=share_link](https://drive.google.com/drive/folders/1CtMhTf-v0nSUSaTJVelILkDMrLfF1U5Y?usp=share_link)
 	- Spanish data: [https://drive.google.com/drive/folders/1SThJ6tyUAzvfVSFo6w_VyB4HPt381jp1?usp=share_link](https://drive.google.com/drive/folders/1SThJ6tyUAzvfVSFo6w_VyB4HPt381jp1?usp=share_link) 
* Jira Link: [https://cyverse.atlassian.net/jira/software/projects/QNLP/boards/27](https://www.google.com/url?q=https://cyverse.atlassian.net/jira/software/projects/QNLP/boards/27&sa=D&source=editors&ust=1717607867016357&usg=AOvVaw2fccm9pIgF5Yw5sAb26eH0)     
* [Miro Whiteboard](https://miro.com/app/board/uXjVKVPCIK4=/?share_link_id=77584526552) 
* Papers:
	* the most fundamental paper which introduces QNLP is 2010 DISCOCAT [paper](https://drive.google.com/file/d/1T7H5WH1q0mKng-zwqOYrlqEkBpOIcUDR/view?usp=sharing)
 	* To undrestand that, you need to understand 1998 Lambeks paper on math===language: [here](https://drive.google.com/file/d/1WmHNND7geQTfO3sRK-NDoOBHtKZL7pAa/view?usp=sharing)
  	* To understand 1998 lambek's work, you need to understand [1958](https://drive.google.com/file/d/1mXmLMH9NbQgrbB550XIaOph07yRV3INO/view?usp=sharing) lambek's work 	
	* 2020 bob coecke [QNLP](https://www.google.com/url?q=https://drive.google.com/file/d/15hXA_ecFN31JJdt9E8POdUFT1mlcwssv/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867017007&usg=AOvVaw2jRw8msgoQEVE_z5vZxQCa)
	* Type grammar revisited: [https://link.springer.com/chapter/10.1007/3-540-48975-4\_1](https://www.google.com/url?q=https://link.springer.com/chapter/10.1007/3-540-48975-4_1&sa=D&source=editors&ust=1717607867017296&usg=AOvVaw0T99YvALpGGqp50dAnYxz9)
	* Khatri et al. thesis: [https://drive.google.com/file/d/141UTEmduZoFhq0-d1peWLGdUhMLpNlho/view?usp=share_link](https://drive.google.com/file/d/141UTEmduZoFhq0-d1peWLGdUhMLpNlho/view?usp=share_link)
* Colab notebooks:
	- Data pipeline: [https://colab.research.google.com/drive/1YwdVkFZRt30QPuUwQ-y9W1vSnYlkS656?usp=sharing](https://www.google.com/url?q=https://colab.research.google.com/drive/1YwdVkFZRt30QPuUwQ-y9W1vSnYlkS656?usp%3Dsharing&sa=D&source=editors&ust=1717607867017671&usg=AOvVaw3sePnYQ_2mwLcqo1YYvu9Y)
	- Lambeq for Spanish, run with Spider parser: [https://drive.google.com/file/d/1wTo8rAObpuLu65DyFo1D0gE5kKjUtzBf/view?usp=sharing](https://www.google.com/url?q=https://drive.google.com/file/d/1wTo8rAObpuLu65DyFo1D0gE5kKjUtzBf/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867017990&usg=AOvVaw1oMypNSQtjg_K-olMfxRnv)
 	- Khatri et al, for Spanish: [https://github.com/ua-datalab/QNLP/blob/megh_dev/OOV_MRPC_paraphrase_task.ipynb](https://github.com/ua-datalab/QNLP/blob/megh_dev/OOV_MRPC_paraphrase_task.ipynb)
  	- Khatri et al, for Uspantekan: [https://github.com/ua-datalab/QNLP/blob/mithun_dev/v2_khatri_thesis_version_which_gave1_mnli_100_runs_end_to_end.ipynb](https://github.com/ua-datalab/QNLP/blob/mithun_dev/v2_khatri_thesis_version_which_gave1_mnli_100_runs_end_to_end.ipynb)

# Working Group Best Practices
- Save all data in the private repository, to prevent leaks: [https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data](https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data)
- Save code to public repository, so it can be opened on Cyverse and run on colab: [https://github.com/ua-datalab/QNLP/blob/main/](https://github.com/ua-datalab/QNLP/blob/main/OOV_MRPC_paraphrase_task.ipynb)
- Cyverse has resource allocations- so all big training done there. Example: 


# How Khatri et al., (espeically the original [code](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py) works
## Sep 26th 2024
Mithun explaining the work flow of khatri's code in a question answer.
- Basics:
- ANy neural network/machine learning model does same thing; i.e given two things a and b, find any patterns that relates a to b. For example if the task is given a ton of emails marked by a human as spam and not spam, train from it so that when the model sees a new email its job is to predict whether it is belonging to spam or not spam. However, during training the model is provided two things, like i just mentioned a, b i.e model(a,b). In this case a will be an email from teh training set, and b will be the corresponding label (spam or not spam) which teh human had decided. Now the job of the model is to find two things a)what is it/what pattern is there in the data that makes this particular email be classified into class spam (for example) b) what is the common patterns i can find inside all the emails which were marked as spam.At the end of the training process, this `learning` is usually represented as a huge matrix, called weight vectors or parameters, which if you really want to know are the outgoing weights that a neuron assigns to the outgoing connection between itself and its neighbors.
- Now with that knowledge lets get into the details of this project
- There are 4 models that are being used in this code
	- Model 1 QNLP Model
 		- First one is the the main QNLP model whose job is to find a relationship between input sentence and the label, for example 	class A (exactly same as a neural network model). For example one of the data we use are 100 sentences in spanish- which are classified into two classes , dancing and education.
	 	- so the job of QNLP is learn during training model(a,b) , where a is a sentence in spanish, and b is the corresponding class label (e.g.Dancing). This is being done in [line 503]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L503)). Note that by the time code reaches line 503 all training is done as far as QNLP model (the first model) is concerned, and it has learned what it takes for a sentence in spanish to belong to the class education (or dancing). (but no prediction or testing is done on the test partition), Or in other words, once the learning/training is complete, just like NN case mentioned above, the system produces a huge matrix called weight matrix to represented what it has learned- the pattern which makes a given spanish sentence to belong to the education classy, say...very similar to the same `what makes an email belong to class spam`. Only difference here is this weight matrix is not really the weight of the neuron in QNLP instead these are called angles of the rotational gates. That is because instead of neurons QNLP model uses internally something called quantum gates.
 		- Note that only difference between QNLP model and a standard neural network is that standard neural network expects input in terms of vectors filled with numbers, while QNLP model expects input in terms of something called circuits. You dont have to worry about it for now.  We use something called [ansatz]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L475))- (which is yet another tool from Quantum physics, which you can ignore as a black box for now) , which internally converts a given sentence to a circuit
  - Qn) great but that is model 1,..what was the other 3 models, and why were they needed? things seem to be almost done so far as far as traininng is concerned right?
  - Ans: very good question. Except, when the author used the aformementioned trained model to predict on the test dataset, he ran into a very weird problem. There were some words in teh testing set, which were not present in the training set. Which means there is no corresponding mapping between this word and an entry in learned matrix aka the weights aka the angles of gates. (Todo for mithun- confirm this. I still dont understand how words can have weights).
  - Anyway, this is called the out of vocabulary problem, which is a solved problem in classical NLP. Rather it took almost a decade to solve this problem. When in 2013 initially this problem occured, people ignored it, mainly because a) it was a very rare phenomenon. Rather the training corpus was so huge that it was very rare that a word wil be encountered in the test set which was not present inthe training set. b) the way people ignored it was by creating a tag/word called UNK (stands for unknown) and assigning all new OOV words that label and its corresponding weights.
  - as you can see that was a disaster waiting to happen. Not all UNK words mean the same. This became a big problem in a low resource seeting (similar to teh 100 spanish examples above ) because when the training data is very small, there is a high probaility that the word in test set is never seen before/ OOV
  - That is when the whole concept called Embeddings were invented. The idea of embeddings is simple. Take every single word in the dictionary and pre-create/beforehand itself create a corresponding vector for it. This is typically done using techniques called CBOW (continuous bag of words) or Skipgrams if youw ant to know exact details.
  - However even that hit a huge block especially in case of languages other than english. For example in spanish, even if the embedding model has seen most of the words in the languaage(e.g. el, para), the testing set might still have an entirely new rare word (e.g., pardo- a color between grey and brown). This was even happening in ENglish. So as a solyution to this problem someone invented a technique called fast text embeddings (which eventually inspired byte pair encoding, which is used in Transformers/LLMs). The intuition is that instead of learning embedding for every single word in say ENglish, they learn embeddings for n grams. For example instead of learning the embedding for a word `arachnophobia' the new model will instead learn embeddigns for `a',`ar',`ara' etc...i.e oen gram, two gram 3 gram etc. The advantage of this approach is that even if an entirely new word is encountered in the testing data, and even if the word was not seen before/part of the embedding model, it can still be built up by the n-grams.
  - Anyway long story short, we also encountered the OOV problem, and we decided to use FastTExt. I.e give every single word to fasttext, and get its corresponding embedding,. This can be seen in line [337](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L337) where the fasttext model is initialized and in line [130]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L130)) where every single word in training and testing is converted to its corresponding embedding.
  - Qn) But i thought you said your QNLP model takes as input some circuits thingie and outputs angles of the gates in these circuits. What will i do if you give me a vector for a word.
  - Ans: Very good questions. This is where model 3 comes up.
  - the author (nikhil khatri) created a simple Neural network based a 3rd model whose only job is to find pattern between embeddings and the angles.
  - Qn) I dont get it. how does that help.
  - Ans; Remember we were saying that by the time the code control executes [line 503]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L503)) only TRAINING part of QNLP model is done. Now say the same model is going to be used for prediction in the test set. What is the meaning of testing. i.e we give same input like in training (e.g. circuits corresponding to a new sentence in test set). Which will inturn be used to multiply with the angles of the gates of the learned model, (equivalent in NN world will be multiplying embedding of a test set ka word with the weights of the learned model), and get a float value, using which the Model decides if the test data point belongs to class A or B 
  
  -  and it encounters an OOV word. So it goes back and asks the 2nd model, the fast text embedding generated model, and asks- here is a new word, can you give me the corresponding angles for it. So the model 2, does exactly that and gives out a vector. So then model 1 asks- WTF am going to with vectors, i only know circuits as inputs. That is where Model 3 comes into picture. So to remind you model 3, is a model which tries to find patterns between model3(a,b) where a is embedding and b is the weight equivalent(mithun todo: Ideally it should have been finding pattern between Embedding and circuit.. am still not completely clear on why model 3 outputs patterns between embeddings and weights/ angles of QNLP model instead of circuits- go find and update here). ANyway what happens is, before model1 does any prediction, we train model 3 between two things, the embeddings coming from fast text for each word in training data set, and the corresponding angles which we get from the TRAINED QNLP model, which is model 1. Specifically, in [line 197 ]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L197))and 198 is where we initialie the inputs a, b to model 3 are created, i.e., the embeddings from fast text and the weights from the QNLP model. Then training of this third model is trained in [line 218]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L218))
  -  Now once the third model is done training
  -  Qn) ok then what does model 4 do
  -  Ans; Short answer it is yet another NN model which is purely used for prediction
  -  Now consider line [248]([url](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L248)) the first thing they do is, take every word in the test vocabulary, and gets the corresponding embedding of it from model 2 and then gives it to model 3 who returns with the correspondingn weight that model 1 understands.
  -   All these weights/angles which is taken out of model 3, is used to initiate model 4.
  -   This model 4 is something which takes test_circuits as inmput (just  like  QNLP model) and predicts output (which is which exactly model 1 does- however, now remember there is shit load of embeddings involved, which is hidden from you)  at happens in [line 264](https://github.com/ua-datalab/QNLP/blob/mithun_dev/archive/master_khetri_thesis%20.py#L264)
  
  -  Thats how the whole system works on 4 models.

# Meeting Notes

## September 30th 2024
- Debug trainer"
	- `sym` explantion- aldea_0__s- word, label, ways in which it can be expressed in lambeq
	- The embeddings from Fasttext model (2nd model initialized) are used as initial parameters of the qnlp model
		- Get QNLP model initialized with embedding weights
	 	- Trying to assess the issue with assigned weights 
	- initial parameter shape mismatch, shape of weights array does not match requirement
 	- Model 3 `.fit()`:
 		- if weights are updated correctly, then there should be no OOV words in train 
 	- Current objective- why is the validation code buggy? Khatri et al. is not evaluating at every epoch, but Mithun was trying to do that
  		- val set has a lot of OOV words, so if it is run along with training set, we will end up with a lot of OOV words and bugs
- What does khatri et al. mean by "weights"?
	- ToDo- Mithun  
-  Readings
   	- Todo: 1999 lambeq grammar, 2010 (start here)    

## September 25th 2024
- Potential solution for OOV words in the Spanish model- why is the model not performing well with embeddings?
	- model is given the word's vector representation. in the case of OOV words,we are providing embeddings
	- Q: are these two types of input in the same format? A NN is multiplying given vector with weights to find correlations 	
- Unlike English, the load of OOV words is very high, so the model will need to find a solution and rely on embdeddings
- In QNLP context, we have circuits and a combination of angles. How do we convert embeddings into a combination of angles?
	- Khartri et al.'s solution- use a neural network model that finds patterns between embeddings and angles, so that this conversion is aided by model making predictions.
	-  Now, we need to test each model to see where the bottleneck is.
	
## September 23th 2024
- Created a flowchart explaining Khatri et al.'s code, to understand each process in the model used
	- Why is there is NN model (MLP) between the input and QNLP model?
		- Section 62- NN is learning a map from embeddings of vocalbulary in training set to QLP model's trained parameter assignment. This helps "generalise unseen vocalbulary".
	 - Does the Uspantekan model without embeddings require an NN at all? No, because there are no embeddings to map.  
	 - What are the input and outputs to the QNLP model with embeddings?
  - Steps for training:
  - For every word in training vocabulary:
  	1. Thread 1:
   		1. Get embeddings from Fastest
   		1. Generate vector representations
   		1. This is a **pre-trained model** 
	1. Thread 2:
  		1. Initialize some non-zero weights for the QNLP model
   		1. In parallel, run the training dataset through lambeq ansatz, and obtain circuits at the sentence level.
   		1. Use these circuits as training data and **train the model** to get weights
   		1. Obtain word-level representations from the model weights. TODO: how do we generate word-levels representations from sentence-level trained weights? 
	1. Bringing it together:
   		1. Create a dictionary by mapping each word in the vocabulary with weights from the QNLP model (which is also a vector representation). DON'T KNOW HOW
   		1. Create list of lists mapping each word to a vector representation of its fasstext ebedding. TODO: check how this is done.
   		1. **Train a model** to find a relationship between word-level vector representation from QNLP model weights, and the embeddings from fasttext
   		1. **Train another model**: TODO- find out why
        1. Testing the final model 



## September 18th 2024
- Discussion of code v7
	- `train.preds()` did not work: Pytorch tensors include the loss values plus the computational graphs, so there would be an issue running them with numpy
 	- Solution: `torch.detach()` which creates a copy of the tensor, without the graph
  - Substitute hard-coded loss function with `torch.nn.loss`
  - Fixed error that combined train, test and dev for generating training data
  - what is `embedding_model`- fasttext
  - ToDo: draw a flow diagram to understand what `qnlp_model` is doing
  - DNN model: training between word and its embedding
  - QNLP model: angle and word
  - ToDo: find difference between symbol and word
  - ToDo Mithun: refactor code to ensure all functions are in the correct order, based on Khatri et al.

## September 16th 2024
- Tech Launch Arizona funding
	- AI + Quantum applications
- QNLP applications in Megh's dissertation
	- LLMs are trained on next sentence prediction, entrainment looks for people sharing linguistic features
- Discussion on QNLP applications:
	- RAG also a good fit for indegenous languages? May be a low-lying fruit, even if it runs into data issues
- Sprint: debugging v7 code
	- What does maxparams do? Spanish Fasttext model provides 300 parameters. So, the number is hard-coded into the code
 	- OOV words were an issue, changed max_params to fix this
  	- Setting up loss code and accuracy code
- Pytorch vs Keras
	- Using pytorch for calculating angles in place of numpy
 	- But if keras is being used   

## September 11th 2024
- Code sprint and logging and documenting bugs
	- Spanish embeddings model:
 		- added v7 to `megh_dev` and adding documentation.
   		- See: [code compare for more](https://github.com/ua-datalab/QNLP/commit/3472a3addd948076677e83c70dfd36892b38c41a) 
		- 1 AI model + MLP for mapping spanish embeddings with spanish text
  		- Initial model fixed by switching from numpy to pytorch

## September 9th 2024
- Uspantek + spanish
	- run more experiments to confirm the results 
   	- cons: Mithun wants to read more before turning knobs.
- write paper:
	- reproduce results again?
 	- confirm with robert, 
 		- we can use the uspantek data.
 		- remind him to connect with his collaborator's phd student.
- upcoming deadlines
	- NLP
		- COLING: sep 17th 2024  
		- ICLR: 2nd oct 2024
		- NAACL: Oct 16th 2024.
	- Computational linguistics
		- SCIL- december 2024
		- [LSA](https://web.cvent.com/event/40d9411e-b965-4659-b9c3-63046eeed3d4/summary)
- update: we decided to start writing paper- kept [here](https://www.overleaf.com/4483532232tcfnfdrrcbdc#12a1b4). AIm is for ICLR 2nd october. But more importantly, it is an exercise to capture all the thoughts fresh in our head
- Paper to-dos
	- Write introduction
	- get a lit review for:
 		- QNLP,
   		- Uspanteko,
		- low-resource language LLM research:
  			- https://arxiv.org/pdf/2406.18895
     		- https://aclanthology.org/2024.americasnlp-1.24.pdf
        	- https://arxiv.org/pdf/2404.18286  
  	- Results and methods
  		- Baseline models for Spanish, Uspantekan
  		-  Future: Fasttext embeddings for Spanish model
- Current issues:
	- Embeddings model has a lot of bugs
 - todo
 	- wed meeting,
  		- Mithun will try to push through code of spanish + embeddings, with the aim of: lets have closure/have 3 systems working
  		- Mithun: find more AI related NSF - QNLP for indigenous languages
   	- 	 

## September 5th 2024: Meeting with Robert
- Dataset has other topics related to education and dance: like "teaching Uspantekan", and other forms of dancing
	- We have more data! 
- [Link to Slides](https://docs.google.com/presentation/d/1jw9_b55BC4HOMmtOaqbXjDkOR8nn9_Zg8xLjK86KG8w/edit)
- Robert's update:
	- Currently: 1800 sentences, 12k tokens, with dependency parsing
	- 10k tokens for different discourse types
	- Plus 5 other languages with spanish dictionary  
	- QNLP has its own parser- but throws out a lot of sentences which it can't parse
	- Super helpful update about pre-parsed sentences!
- Target: which NSF project should we consider?
      - "Dynamic language infrastructure and Language Documentation" grant currently funds Robert, along with CISA (proposed by NSF).
	- Maybe a good option
	- Target date: Feb 18th 2025 [link](https://new.nsf.gov/funding/opportunities/nsf-dynamic-language-infrastructure-neh)
	- Better than AI grants, dominated by LLM
	- NSF doesn't like to fund the same grant twice- so keep both projects meaningfully different!
		- Check if QNLP can be imagined as an extension or addition to an NSF grant
		- Extension with more money- check if it's a thing for linguistics-focussed NSF, Robert could also get more funding. 
- Moving away from dataset creation grant, to a new theoretical framework (QNLP)
	- Work on more languages, so focus is on low-resource languages generally
 	- Another important focus: focussing on why this technology is a good fit for a particular use-case (low resource languages), and target tasks that human annotators are really struggling with
- Ex. automating: morphological analysis, parsing, POS taggging, spell-checkers, word search 
- What is the ask in terms for funding?
	- Personel: funding for Mithun, an RA/postdoc
	- Robert's experience: his share is 135k, most of it goes to the collaborator's funding, plus annotators
- Other next steps: check out of the box QNLP can help with tagging
   - Compare how the current AI automatic tagger trained by Robert's collaborator does compared to this
   - Mithun: read Robert's grant

## September 4th 2024
- v4 code running end-to-end. Why?
	- Code was stuck before `fit()`.
 	- Code switched from `numpy` to `pytorch`, needed to switch from arrays to tensors
  	- Ansatz- issues
  	- Parser language settings- when the parser encodes
  		- Embeddings from fasttext was different from what `lambeq` expected:
  	 		- Parser from `lambeq` adds underscores and to each entry, which is missing in the fasttext embeddings. Needed cleanup to prevent mismatch.
		- Khatri et al. expects a double underscore at the end of each entry
	- Padding for symmetrical arrays that `numpy` needed. So, switched to Pytorch models. Hopefully, quantum models will also not have this issue.
 	- Expected issue, as no one has used spanish embeddings.
  - Big picture- end-to-end model for Uspantekan and Spanish
- Notes for meeting with Robert
-  Current baselines: with about 100 sentences as input
	- Spanish:
 		- without embeddings, with 100 sentences, classification accuracy is 71%. Very tunable
 		- Next plan: see what classification accuracy we get with embeddings
   	- Uspantekan
   		- Uspantekan data, no embeddings
   	 	- Classification accuracy: 72%
- Next steps:
	- see what classification accuracy we get with Spanish embeddings, to see if embeddings can improve scores. This will help us rely on non-English text
 	- Tuning
  	- Get F1 scores to assess all quadrants of the testing
  	- Assess with quantum computing is able to get us closer to underspecificity.

## August 28th 2024
- Why Spider? It works, gives results- use it for Spanish, as well as uspantekan
	- Use for both cups and ansatz
	-  
- Model
	- Use BCE loss- original code coded it, instead of calling it
 - To Do
 	- v6 code- lambeq and pytorch have version issues
  	- Do `lightning` and `lambeq` work together?
   - try with v6 code and pytorch
    	-getting the symbol doesnt have size issue in laptop. Sounds like a version mismatch between lambeq and pytorch
    	- try on colab
      	- update- gets the same error on colab
       	- try v4 on colab.-update: works fine. 
        - also try v4 on laptop
        - if no error related to pytorch in trainer.fit:
        - 	update@august31st-OOV error, . fixed by taking .fit() out of wandb sweep.v4 ran end to end for both spanish and uspantekan- atleast till trainer.fit since v4 didnt have the second ML model that khatri uses
        - 	find why v6 is not running.
        - 	update@august31st-v6 still giving OOV error in first model
        - 	remember: goal here is to get the spanish to work end to end with spanish embeddings. t
        - 	then we willt hink about aligning with uspantekan translations for OOV in uspantekan
        - 	
- 	else:
        - switch to Quantum Model- using actual quantum computer. If we are fighting stupid infrastructure and dll issues might as well do it for quantum model, not stupid pytorch or numpy models.
  	
## August 26th 2024
1. try with NUMPy model and square matrix issue
	-  try with making all sentences padded with . -- failed -bob cat parser, automatically removed . 
	-  try with same sentence- works fine./no matrix/array difference issue
	-  sentence level: 
	-  diagram level: ignored
	-  circuit level: tried adding dummy circuit. i.e say XX gates back to back ==1 but became a pain since they wanted it in Rx Ry gate. 
		- why was this not a problem earlier
   		- why 10?
- why did this not happen in the english version- or even uspantekan version?- our own code?
	- in khatri tehsiswas he terminating english sentence- go back and look at his original code- answer: no, he is also doing same maxlength <=
- what he is doing with maxlen.- picks sentences less than maxlength. 
2. what are the potential solutions
	- without quantum level change
	- try with period.--failed
	- try with filler words. uhm--failed
	- tokenizer spanish
	- how is our own english/uspantekan code different than the spanish one. Are we using a different spider?
	- spacy tokenizer
3. update. we decided to do this comparison first. i.e compare between v4 (the code which worked end to end for uspantekan) kept [here](https://github.com/ua-datalab/QNLP/blob/mithun_dev/v4_load_uspantekan_using_spider_classical.py) and v6(the code which is not working for spanish) [here] (https://github.com/ua-datalab/QNLP/blob/mithun_dev/v6_qnlp_uspantekan_experiments.py)
	- how is khatri's code different than the spanish one. Are we using a different spider?
	- with quantum level change
4. Replace with quantum model/simulation?
5. once we have end to end system running, to improve accuracy, add a spanish tokenizer expliciity

## August 21st 2024
- Main bug- Lambeq's classical example uses Numpy, which is set up to require square matricies as input.
	- Khatri et al. uses the first 10 words in the sentence, discards the rest.
 	- Code does not run when sentences have >10 words
- Potential Solutions
	- try padding sentences with special characters or filler words?
  		- This did not workwith special characters, which got filtered out
  	- Filler words like "um"
  	- Choose n_words >10, based on the data?
  		- ToDo Megh: work on this
 - Mithun contacted Robert Henderson, requested meeting  

## August 9th 2024
- Quick discussion on the difference between raw embeddings and other options
- ToDo Tuesday: create materials for student presentation, and progress in the QNLP project
- Debugging the code to fix fasstext issues
	- The print statements are printing circuits, making it hard to see which words are the issue
 	- How are OOV words identified and their no. calculated? Is Fasttext being implemented correctly?
	-  not the main issue- embeddings are being used correctly    
  	- Is the code using the fasttext embeddings at all? or the spacy spanish parser?
  	- Updates to code- added a try-except chunk: when a word is really OOV, the code will stop and print details for us
  	- Examine why training loop is utilizing only 14 out of 90 sentences, why is bobcat parser not working with the rest?
  	- Embeddings not being passed in the right format- hence a shape error

## July 30th 2024
- General discussion for wrapping up summer responsibilities- and plan going forward
- Close reading of section 6.1-6.3
- skip gram model vs context-aware methods
- Mapping fasttext embeddings and the given input

## July 25th 2024
- Looked at the Khatri et al. code for Spanish, and worked on code fix: https://github.com/ua-datalab/QNLP/tree/megh_dev
	-  Most of the word are OOV, so they can't be put into grammatical categories
	-   ToDo: Assess what khatri et al. does with OOV cases (section 6)
	-   ToDo: run code on VSCode
- General Discussion:
	-  How does Quantum trainer compare to NN?
 		- Feed forward- we look at the loss between the original and predicted value, does back propagation, until the right combination of weights provides us useable prediction
  		- Instead of neurons, we use quantum circuits
    		- Instead of parameters for gates, we have a different system
      		- Objective- minimize loss, and get optimal assignment of parameters, best combo of weights, to find mapping between input setence and label. "Why does a group pf words belong to x category, not y"?
        	- What is loss value?
        		- Classical: diff between gold and predicted, we backprop
          		-  QNLP- Instead of weights, think of angles. machine's job is to find optimal "angles"  
  		- Thus, the trainer for QC is the same as NN- both have similar black boxes
    			- Very hard to explain the difference between two sentences with the same syntax, but different meanings.
      		- Different words will have different angles- so we can explain semantic differences in syntactically identical sentences
		- The code should look the same for deep and quantum trainer
- What is "OOV", from a coding perspective?
	- out of vocabulary words are assigned the same weights, which is not an accurate way to proceed
	- Fast text, Byte Pair Encoding: ways to solve this problem by using embeddings (n-gram models) to assign different weights to different OOV words
 		- Our code needs to learn some kind of mapping between ngrams and angles  
	- In our code, nearly all words are assigned OOV labels

## June 30th 2024
- Discussion of Khatri et al., section 6
	- Continuous bag of words model- teaching a machine a simple pattern matching method. When X word exists, works Y, Z are also likely to occur.
 	- "meaning is context", not "context is meaning".
  	- Word embeddings have a similar understanding of mental lexicon as expeiments on lexical semantics?
  	- Fast text- improving OOV issues by either working with n-grams (thus, meaning agnostic)
  	- Mithun's idea- use GPT embeddings which may take care of OOV words by assessign words in a network of related words, rather than related phones.
  	- 6.2- words belonging to one topic or category will be seen together. Two models are used, a general embedding, as well as a perceptron trained on task-specific vocabulary
  	- Implementation of the baseline models- deeper models performed better than surface models
  - ToDo- look at 6.0-6.4 again, and come back with notes. 

## June 25th 2024
- Pivot to actually determining what the LLM baseline classification accuracy is for our dataset, os that we know what the quantum approach needs to beat.
- ToDo Megh:
	- Move Mithun's single python file to our repo
 	- Edit to load Uspantekan data 
	- Run the LLM code, an untrained DistilBERT and RoBERTa using Mithun's codebase
	- Report bugs and/or result.
- Mithun tries to load embeddings + quantum
  
## June 21st 2024
- Updates to khatri et al code:
	- Current work on Spanish data, using khatri et. al.: [https://github.com/ua-datalab/QNLP/tree/megh_dev](https://github.com/ua-datalab/QNLP/tree/megh_dev)
 	- Mithun shared his updated code for khatri et. al., that works on Uspantekan:  https://github.com/ua-datalab/QNLP/blob/mithun_dev/v4_load_uspantekan_using_spider_classical.py
  - 
  - Overhauled code to fit our classification task that has only one feature vector, as opposed to two. (that is because khatri code was designed for NLI kind of tasks, which expects a pair like hypotehsis and premise)+ `lambeq` libraries and modules needed to be replaced due to depreciation.

## June 7th 2024

* How to access shared analyses on Cyverse- Mithus shares his setup so we have access to GPUs
* Go to [https://de.cyverse.org/analyses/](https://de.cyverse.org/analyses/) and switch the dropdown menu to "shared with me"
* Choose the relevant analysis.
* Setup and tech issues
	* Get permission to run analysis with a GPU on cyverse. Talk to Michele about permissions
 	* Set CPU to 8GB 

## June 5th 2024
* [results location]([url](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing))
1. Can qnlp + uspantekan- straight out of the box give a good classification accuracy?
	* update: NO
	* Rather after tuning max accuracy on dev was 72%...which was given by cups and stairs model
1. Options
	1. path 1:
		1. go back to  spanish- and load with embeddings.
	2. path 2: try with discocat/bobcar parser + uspantekan.. last time we tried, got errors.
* Updated Jira with new tasks and updates
* Megh's updates:
	* Set up repo for code, as well as a folder with dataset in rhenderson's repo:
 	* QNLP dtaset: https://github.com/bkeej/usp_qnlp/tree/main/qnlp-data
 	 * QNLP code repo: https://github.com/ua-datalab/QNLP/blob/main/Project-Plan.md	 
* Todo for Megh
	* Run the code for Spanish- we will need embeddings
	* Use fasttex- Used by GPT for building ngrams and aligned work vectors. 
	* Thesis code from Mithun’s contact (Khatri et al.)
		* Run this code directly and see what happens
* Examined issues received outlook calendar nvotes

## May 30, 2024 :
* Jira setup for project
* Classical model pipeline with Spanish up and running
	- Major update- removed all sentences with >32 tokens, and the Spanish model was able to run with lampeq
	- Training accuracy is good, but loss is too high
	- Needs finetuning
* What is the language model in th classical case doing?
	- Spanish SpaCy model is using the tokenizer to be able to use word-level knowledge.
	- Then, bobcat uses the sentence2diagram method on the result.

* New proposal- to run Spider directly on the uspantekan sentences
* ToDo- move notes to Github, create notes.md and keep all data on Github
* Jira for project setup-
	- Todo- assess the space and make sure Megh knows her tasks.
	- Check out QNLP52
* Paper writing
	* Pivot from a whitepaper to a one-pager
		* Motivation, contribution thought

## May 7th, 2024:
* Classical case:
	- Filename non-ASCII issue resolved with rename   and train, test, dev splits saved
	- Classical case-  [ran into error while running sentence2diagram](https://www.google.com/url?q=https://colab.research.google.com/drive/12kNxLNX162hGznIYenBSqLJbflmFaE1y?usp%3Dsharing&sa=D&source=editors&ust=1717607867019672&usg=AOvVaw1SAvjipfXEAOkwHKcnRCgQ)
	- Check classical case with Spanish, assign spanish SpaCY language model to pipeline if needed
	- ToDo for Mithun
	- Fix classical trainer, section “Training the model”, cell 67
* Whitepaper updates: Mithun shared his current working document

## April 19th, 2024:
* Issue with non-ASCII characters in filename persists- can’t read from file
* Pipeline for reading sentences ready- need to work on reading files in with non-ASCII filenames
* Todo:
	- Mithun- set up an overleaf for a paper draft
	- Mithun- find out how to run QUANTUM CASE on Cyverse
* Data pipeline todo \[Megh\]:
	- randomize sentences, and create test, val, train splits
	- Convert the sentences to a dataset in the format: Label\[Tab\]Text\[Tab\].
		+  0 for bailar, 1 for educacion
* ToDo Mithun: create list of file names, and convert them to ASCII

## April 11th, 2024:

* Updates
	- Classical case set up on Cyverse
	- Data cleaning pipeline setup
* ToDo: upload code to Colab \[DONE\]
* Todo:
	- Mithun- set up an overleaf for a paper draft
	- Mithun- find out how to run QUANTUM CASE on Cyverse
	- Megh- find a way to run data pipeline on Cyverse \[DONE\]
	- Megh- complete data cleanup pipeline \[DONE\]
* Proof of concept:
	- Data- done
	- Data cleanup pipeline: done
	- Code- quantum case in process
* Data pipeline todo: \[DONE\]
	- randomize sentences, and create test, val, train splits
	- Convert the sentences to a dataset in the format: Label\[Tab\]Text\[Tab\].
		+  0 for bailar, 1 for educacion
	- ToDo Mithun: create list of file names, and convert them to ASCII
* QNLP pipeline setup
	- ToDo Megh: Run classical case on Cyverse with Spanish data

## April 2nd, 2024:

1. We went through learning
	1. Bobcat parser
	2. Convert text to diagrams
	3. Convert diagrams to circuits
	4. Rewriter+ reduce cups
	5. Go through full fledged training example of classification of IT or ood
2. Todo \[Megh\] for next week:
	1. Set up and Try out lambeq classification [task](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-classical.html&sa=D&source=editors&ust=1717607867023254&usg=AOvVaw3Se6n9TAlMX5oyEHCO2C1Z)  on Cyverse
	2. With spanish text from the Robert henderson’s data
		1. Pick 2 classes (i.e file names in the data directory)
			1. dancing/bailes
			2. [Education](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp/blob/main/data/UD/Acontecimientos_sobrenaturales.conllu&sa=D&source=editors&ust=1717607867023895&usg=AOvVaw3gzoN1EoQiznA9p15PbxWz)
			3. Try to recreate the same ML pipeline shown in the lambeq task above.
				1. Using spacy spanish tokenizer.
3. Concrete steps:
	1. Download code from the Lambeq tutorial’s   [Quantum](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html&sa=D&source=editors&ust=1717607867024391&usg=AOvVaw0hSG13wlkmHZP5akrEMoPD)  case
	2. Replace [training](https://www.google.com/url?q=https://github.com/CQCL/lambeq/blob/main/docs/examples/datasets/rp_train_data.txt&sa=D&source=editors&ust=1717607867024688&usg=AOvVaw0C4Tf2Ane5m0bQGPVI7Mj4)  data from relative clauses to the text classification task (see ‘Classical Case’ [https://cqcl.github.io/lambeq/tutorials/trainer-classical.html](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-classical.html&sa=D&source=editors&ust=1717607867024954&usg=AOvVaw0iw7KHxbXYMsbUCXhl0ft6)
	3. Run the code and assess performance.
	
	4. Lambeq- removed “glue” words, or ones that don’t add to the semantics of a sentence. these correspond to the stop words of a language
	5. Colab notebook: [https://colab.research.google.com/drive/1krT2ibzrfLxin6VT5-nyXWr8\_HGEHNVF?usp=sharing](https://www.google.com/url?q=https://colab.research.google.com/drive/1krT2ibzrfLxin6VT5-nyXWr8_HGEHNVF?usp%3Dsharing&sa=D&source=editors&ust=1717607867025382&usg=AOvVaw3m2tOfqrp4UBXV1cY_-H13)
	6. research ansatz
		1. Advantage- it is able to better capture the richness of meaning
	7. Current status- spanish spacy tokenizer is able to tokenize correctly for Spanish. However, QBIT assignment is not working.
		1. How do we initialise the qbits?

8. Pipeline for the task:
	1. Use the same task as tutorial: [https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html](https://www.google.com/url?q=https://cqcl.github.io/lambeq/tutorials/trainer-quantum.html&sa=D&source=editors&ust=1717607867025982&usg=AOvVaw2jfGhkUkfrlNbkzWkBIR2t)
	2. data- 100 spanish sentences from the uspantecan corpus
	3. ToDo- plan a classification task for uspantecan
	4. Robert Henderson’s repo: [https://github.com/bkeej/usp\_qnlp](https://www.google.com/url?q=https://github.com/bkeej/usp_qnlp&sa=D&source=editors&ust=1717607867026388&usg=AOvVaw0W2XkIkcQV3xFiThDTApyS)
	5. We select 2 data files, with different topics
	6. We create a classification task for differentiating between sentences under each topic

## Mar 19th, 2024

* What is [DisCoCat](https://www.google.com/url?q=https://cqcl.github.io/lambeq/glossary.html%23term-DisCoCat&sa=D&source=editors&ust=1717607867026871&usg=AOvVaw1JT_SF8YpM08KtOmjl2vs9) ?
	- Discrete mathematical category
* How do we choose a parser?
	- BobCat parser is the most powerful, so that’s the one bing used.
* Spider reader- we are not trying to build a bag of worlds model. We want to keep the grammar
	- It is faithful to lambeq
* Fix error with unknown word handling
* Monoidal structure-
* Start code for Spanish
	- Can bobcat work with Spanish?
	- how will PoS
* Step 3 Parameterization-
* Import Robert’s data
* ToDo- how to collaborate on a jupyter notebook cyverse
* ToDo- compile their code locally- edit SpacyTokeniser to change language from English to Spanish. Mithun is discussing this with the lambeq team
* ToDo- set up the dataset from Github to Cyverse, and use spanish translations.

# Mar 12th, 2024

* 1998 Lambeks paper on math===language: [here](https://www.google.com/url?q=https://drive.google.com/file/d/1BWhs5zOoA2n7y8aUnKoamfift0t9Xdhu/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867028080&usg=AOvVaw1KcGOV2dbqQGcyH1m-yBwy)
* 2020 bob coecke [QNLP](https://www.google.com/url?q=https://drive.google.com/file/d/15hXA_ecFN31JJdt9E8POdUFT1mlcwssv/view?usp%3Dsharing&sa=D&source=editors&ust=1717607867028363&usg=AOvVaw0YfZ4zJY2KeDRyG4ZxWA1c)
* Type grammar revisited: [https://link.springer.com/chapter/10.1007/3-540-48975-4\_1](https://www.google.com/url?q=https://link.springer.com/chapter/10.1007/3-540-48975-4_1&sa=D&source=editors&ust=1717607867028593&usg=AOvVaw3TiubEsWFejcv65hog7NXV)
* [https://cqcl.github.io/lambeq/](https://www.google.com/url?q=https://cqcl.github.io/lambeq/&sa=D&source=editors&ust=1717607867028793&usg=AOvVaw2RpORwY67SCqRm2u84fRef)
* Neural networks use stochastic gradient descent- a top down approach. Language needs a bottom up approach
* Category theory hierarchy- everything is a category.
* Lambek- The Mathematics of Sentence Structure
	- Language is a bag of things, with 3 things- noun, sentence, NP. Anything can be created from these three
* Type Grammar revisited (Lambek 1999)
	- groups and proto groups- when an operation is done on members of a group, it remains in the same group?
* Combinatory Categorical Grammar
* Qubit- every time a decision is made, multiple options are collapsed into one. Until a decision is made, all possibilities are true. A qubit hangs out in infinite space, until it is acted upon by an operator.
* Quantum model for NLP Coecke
	- one grammatical category acting on a word, to move it one way or another
	- Dependency parsing looks like matrix multiplication
* Bag of words
	- sentences split into smaller meaning carrying chunks (words), which can be interchangeably combined in different ways
	- However- word combination is governed by semantic relationships between words
* Lambeq- pip install lambeq  to install
* If we know the minima of the gradient descent- can we build language up from it?
* TODO- install lambeq  and feed it a sentence \[done on Cyverse\]
* Run end to end- work on it like a tutorial
* Think- tokenizer available for English, Spanish, but not other languages. How do we work without one?
	- Run this on Spanish first
	- Think of a problem
* Jupyter notebook stored at: /data-store/iplant/home/mkrishnaswamy/qnlp

# Project plan 
Goal: we want to show Robert a proof of concept that QNLP can work with uspantekan- limited resources- and still give good accuracy
1. Can qnlp + uspantekan- straight out of the box give a good classification accuracy- if yes:
	1. Path 1 bottom up:  
		1. Pick one thread. Eg. spider
			1. Trying on spanish
			2. Find embedding spanish
				1. Split by space- what is accuracy
				2. Try splitting - Spanish tokenizer- did accuracy improve
			3. Align with embedding uspantekan
			4. Run classification task again on uspantekan
	2. Path 2
		1. Qn- why don‘t we straight Run classification task again on uspantekan -with spider
			1. No parser which can break tokens faithfully
			2. No embeddings directly for uspantekan
			3. How much is bare bones accuracy?
			4. With tuning how much can you get it upto?
			5. If they both fail,
			6. Then yes, we can think of bringing in spanish embeddings.
			7. Todo
				1. Train dev of uspantekan
				2. Modifying
2. update: june 5th 2024
	1. Path 2: Ran experiment. [here](https://docs.google.com/spreadsheets/d/1NBINiUsAdrqoO50y_CX_BGGgXcP9Zt6i5nYKvuB70Tg/edit?usp=sharing) are the results of trying Uspantekan with spider parser, bobcat parser, and using pytorch trainer. Rather after tuning max accuracy on dev was 72%...which was given by cups and stairs model -so we have decided to move on.
	2. Options to explore next
		1. path 1: go back to  spanish- and load with embeddings.
		2. path 2: try with discocat/bobcar parser + uspantekan.. last time we tried, got errors...
3. update: june 25th 2024:
	- still working on QNLP +uspantekan + embedddings. reevaluated goal and pivoted because of the question: what baseline
	- are we trying to beat. Decided we will do the baseline first on LLMs
      
# General correspondence:
- why did we decide to go with spanish first and not Uspanthekan?
	- Lambeq pipeline seems to have a language model requirements and needs embeddings. We have some for Spanish, none for Uspantekan
	- We have direct Uspanteqan-Spanish translations, but not English-Uspanteqan. Which means that if things fail, we have no way to examine what happened if we used an English model.
- How many hours can Megh work on QNLP?
	- This semester: 5hrs, as workshop is also a responsibility
 	- Two 10hr projects may derail dissertation
- Spring 2025: 12/8 split will work, as fewer hours needed for creating content    
