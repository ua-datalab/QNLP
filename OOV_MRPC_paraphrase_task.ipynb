{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/QNLP/blob/megh_dev/OOV_MRPC_paraphrase_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lambeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZb00jLWj6lI",
        "outputId": "affd5c55-f65e-4a3b-83ad-d9b983e63453"
      },
      "id": "IZb00jLWj6lI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lambeq\n",
            "  Downloading lambeq-0.4.1-py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from lambeq) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lambeq) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from lambeq) (9.4.0)\n",
            "Collecting pytket>=0.19.2 (from lambeq)\n",
            "  Downloading pytket-1.29.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lambeq) (6.0.1)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.10/dist-packages (from lambeq) (3.7.5)\n",
            "Collecting tensornetwork (from lambeq)\n",
            "  Downloading tensornetwork-0.4.6-py3-none-any.whl (364 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.3/364.3 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from lambeq) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from lambeq) (4.41.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (1.25.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->lambeq) (2.8.2)\n",
            "Requirement already satisfied: sympy~=1.6 in /usr/local/lib/python3.10/dist-packages (from pytket>=0.19.2->lambeq) (1.12.1)\n",
            "Collecting lark~=1.1 (from pytket>=0.19.2->lambeq)\n",
            "  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy~=1.13 (from pytket>=0.19.2->lambeq)\n",
            "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.8.8 in /usr/local/lib/python3.10/dist-packages (from pytket>=0.19.2->lambeq) (3.3)\n",
            "Requirement already satisfied: graphviz~=0.14 in /usr/local/lib/python3.10/dist-packages (from pytket>=0.19.2->lambeq) (0.20.3)\n",
            "Requirement already satisfied: jinja2~=3.0 in /usr/local/lib/python3.10/dist-packages (from pytket>=0.19.2->lambeq) (3.1.4)\n",
            "Collecting types-pkg-resources (from pytket>=0.19.2->lambeq)\n",
            "  Downloading types_pkg_resources-0.1.3-py2.py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.2 in /usr/local/lib/python3.10/dist-packages (from pytket>=0.19.2->lambeq) (4.12.2)\n",
            "Collecting qwasm~=1.0 (from pytket>=0.19.2->lambeq)\n",
            "  Downloading qwasm-1.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (2.7.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->lambeq) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->lambeq) (3.15.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->lambeq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.1->lambeq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->lambeq) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.1->lambeq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork->lambeq) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork->lambeq) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->lambeq) (0.23.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->lambeq) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->lambeq) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->lambeq) (0.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2~=3.0->pytket>=0.19.2->lambeq) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0->lambeq) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->lambeq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->lambeq) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.2->lambeq) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->lambeq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->lambeq) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->lambeq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->lambeq) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy~=1.6->pytket>=0.19.2->lambeq) (1.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->lambeq) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->lambeq) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (7.0.4)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0->lambeq) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0->lambeq) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->lambeq) (0.1.2)\n",
            "Installing collected packages: types-pkg-resources, scipy, qwasm, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, tensornetwork, pytket, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lambeq\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "Successfully installed lambeq-0.4.1 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytket-1.29.2 qwasm-1.0.1 scipy-1.14.0 tensornetwork-0.4.6 types-pkg-resources-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8d883e1",
      "metadata": {
        "id": "f8d883e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "RLSGRQ-HFQh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513a7b55-ebc9-4938-af72-23e0ec7f344b"
      },
      "id": "RLSGRQ-HFQh-",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lambeq.text2diagram.tree_reader import BobcatParser\n",
        "import lambeq\n",
        "\n",
        "parser= BobcatParser()\n"
      ],
      "metadata": {
        "id": "xb8TVmjDFRbi"
      },
      "id": "xb8TVmjDFRbi",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b5a96723",
      "metadata": {
        "id": "b5a96723"
      },
      "outputs": [],
      "source": [
        "MAXPARAMS = 108\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "tE2sI1DHFYT_"
      },
      "id": "tE2sI1DHFYT_",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fdf7d61b",
      "metadata": {
        "id": "fdf7d61b"
      },
      "outputs": [],
      "source": [
        "# Helper functions from khatri et. al.:\n",
        "\n",
        "loss = lambda y_hat, y: -np.sum(y * np.log(y_hat)) / len(y)  # binary cross-entropy loss\n",
        "acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting\n",
        "\n",
        "eval_metrics = {\"acc\": acc}\n",
        "\n",
        "def generate_initial_parameterisation(train_circuits, test_circuits, embedding_model, qnlp_model):\n",
        "\n",
        "    # Note that in this vocab, the same word can have multiple types, which each occur separately\n",
        "    train_vocab = {symb.name.rsplit('_', 1)[0] for d in train_circuits for symb in d.free_symbols}\n",
        "    test_vocab = {symb.name.rsplit('_', 1)[0] for d in test_circuits for symb in d.free_symbols}\n",
        "\n",
        "    print(len(test_vocab.union(train_vocab)), len(train_vocab), len(test_vocab))\n",
        "    print(f'OOV word count: {len(test_vocab - train_vocab)} / {len(test_vocab)}')\n",
        "\n",
        "    n_oov_symbs = len({symb.name for d in test_circuits for symb in d.free_symbols} - {symb.name for d in train_circuits for symb in d.free_symbols})\n",
        "    print(f'OOV symbol count: {n_oov_symbs} / {len({symb.name for d in test_circuits for symb in d.free_symbols})}')\n",
        "\n",
        "    max_word_param_length = max(max(int(symb.name.rsplit('_', 1)[1]) for d in train_circuits for symb in d.free_symbols),\n",
        "                            max(int(symb.name.rsplit('_', 1)[1]) for d in test_circuits for symb in d.free_symbols)) + 1\n",
        "    print(f'Max params/word: {max_word_param_length}')\n",
        "\n",
        "    train_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in train_vocab}\n",
        "    test_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in test_vocab}\n",
        "\n",
        "    initial_param_vector = []\n",
        "\n",
        "    for sym in qnlp_model.symbols:\n",
        "        wrd, idx = sym.name.rsplit('_', 1)\n",
        "        initial_param_vector.append(train_vocab_embeddings[wrd][int(idx)])\n",
        "\n",
        "    qnlp_model.weights = np.array(initial_param_vector)\n",
        "\n",
        "    return train_vocab_embeddings, test_vocab_embeddings, max_word_param_length\n",
        "\n",
        "\n",
        "def generate_OOV_parameterising_model(trained_qnlp_model,\n",
        "                                      train_vocab_embeddings,\n",
        "                                      max_word_param_length):\n",
        "\n",
        "    trained_params_raw = {symbol: param for symbol, param in zip(\n",
        "                                                  trained_qnlp_model.symbols,\n",
        "                                                  trained_qnlp_model.weights)}\n",
        "    trained_param_vectors = {wrd: np.zeros(max_word_param_length)\\\n",
        "                             for wrd in train_vocab_embeddings}\n",
        "    print(\"trained_params_raw: \", np.shape(trained_params_raw))\n",
        "    print(\"trained_param_vectors:\", np.shape(trained_param_vectors))\n",
        "    for symbol, train_val in trained_params_raw.items():\n",
        "        wrd, idx = symbol.name.rsplit('_', 1)\n",
        "        trained_param_vectors[wrd][int(idx)] = train_val\n",
        "\n",
        "    wrds_in_order = list(train_vocab_embeddings.keys())\n",
        "\n",
        "    NN_train_X = np.array([train_vocab_embeddings[wrd] for wrd in wrds_in_order])\n",
        "    NN_train_Y = np.array([trained_param_vectors[wrd] for wrd in wrds_in_order])\n",
        "\n",
        "    print(\"dimensions of features: \", np.shape(NN_train_X))\n",
        "    print(\"dimensions of labels: \", np.shape(NN_train_Y))\n",
        "\n",
        "    OOV_NN_model = keras.Sequential([\n",
        "      layers.Dense(int((max_word_param_length + MAXPARAMS) / 2), activation='tanh'),\n",
        "      layers.Dense(max_word_param_length, activation='tanh'),\n",
        "    ])\n",
        "\n",
        "    OOV_NN_model.compile(loss='mean_absolute_error', optimizer=keras.optimizers.Adam(0.001))\n",
        "\n",
        "    # Embedding dim!\n",
        "    OOV_NN_model.build(input_shape=(None, MAXPARAMS))\n",
        "\n",
        "    hist = OOV_NN_model.fit(NN_train_X, NN_train_Y, validation_split=0.2, verbose=0, epochs=120)\n",
        "\n",
        "    print(f'OOV NN model final epoch loss: {(hist.history[\"loss\"][-1], hist.history[\"val_loss\"][-1])}')\n",
        "\n",
        "    plt.plot(hist.history['loss'], label='loss')\n",
        "    plt.plot(hist.history['val_loss'], label='val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return OOV_NN_model\n",
        "\n",
        "\n",
        "def evaluate_test_set(pred_model, test_circuits, test_labels, trained_params, test_vocab_embeddings, max_word_param_length, OOV_strategy='random', OOV_model=None):\n",
        "\n",
        "    pred_parameter_map = {}\n",
        "\n",
        "    # Use the words from train wherever possible, else use DNN prediction\n",
        "    for wrd, embedding in test_vocab_embeddings.items():\n",
        "        if OOV_strategy == 'model':\n",
        "            pred_parameter_map[wrd] = trained_params.get(wrd, OOV_model.predict(np.array([embedding]), verbose=0)[0])\n",
        "        elif OOV_strategy == 'embed':\n",
        "            pred_parameter_map[wrd] = trained_params.get(wrd, embedding)\n",
        "        elif OOV_strategy == 'zeros':\n",
        "            pred_parameter_map[wrd] = trained_params.get(wrd, np.zeros(max_word_param_length))\n",
        "        else:\n",
        "            pred_parameter_map[wrd] = trained_params.get(wrd, 2 * np.random.rand(max_word_param_length)-1)\n",
        "\n",
        "    pred_weight_vector = []\n",
        "\n",
        "    for sym in pred_model.symbols:\n",
        "        wrd, idx = sym.name.rsplit('_', 1)\n",
        "        pred_weight_vector.append(pred_parameter_map[wrd][int(idx)])\n",
        "\n",
        "    pred_model.weights = pred_weight_vector\n",
        "\n",
        "    preds = pred_model.get_diagram_output(test_circuits)\n",
        "\n",
        "    return loss(preds, test_labels), acc(preds, test_labels)\n",
        "\n",
        "\n",
        "def trained_params_from_model(trained_qnlp_model, train_embeddings, max_word_param_length):\n",
        "\n",
        "    trained_param_map = { symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}\n",
        "    trained_parameterisation_map = {wrd: np.zeros(max_word_param_length) for wrd in train_embeddings}\n",
        "\n",
        "    for symbol, train_val in trained_param_map.items():\n",
        "        wrd, idx = symbol.name.rsplit('_', 1)\n",
        "        trained_parameterisation_map[wrd][int(idx)] = train_val\n",
        "\n",
        "    return trained_parameterisation_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "69b2ff1d",
      "metadata": {
        "id": "69b2ff1d",
        "outputId": "edd991f3-d293-4861-d240-e584022dd98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 3\n"
          ]
        }
      ],
      "source": [
        "# Load data and extract features and labels for train and test sets:\n",
        "import string\n",
        "\n",
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "with open(\"./uspantan_train.txt\", encoding='utf-8-sig') as f:\n",
        "    for line in f:\n",
        "        procd_line = line.strip().split('  ')\n",
        "        train_X.append(procd_line[1])\n",
        "        train_y.append(int(procd_line[0]))\n",
        "\n",
        "test_X = []\n",
        "test_y = []\n",
        "\n",
        "with open(\"./uspantan_test.txt\", encoding='utf-8-sig') as f:\n",
        "    for line in f:\n",
        "        procd_line = line.strip().split('  ')\n",
        "        test_X.append(procd_line[1])\n",
        "        test_y.append(int(procd_line[0]))\n",
        "\n",
        "\n",
        "MAXLEN = 10\n",
        "\n",
        "\n",
        "filt_train_X = []\n",
        "filt_train_y = []\n",
        "\n",
        "filt_test_X = []\n",
        "filt_test_y = []\n",
        "\n",
        "ctr_train = 0\n",
        "for label, s in zip(train_y, train_X):\n",
        "    if len(s.split(' ')) <= MAXLEN:\n",
        "        ctr_train += 1\n",
        "        filt_train_X.append(s.translate(str.maketrans('', '', string.punctuation)))\n",
        "        this_y = [0, 0]\n",
        "        this_y[label] = 1\n",
        "        filt_train_y.append(this_y)\n",
        "\n",
        "ctr_test = 0\n",
        "for label, s in zip(test_y, test_X):\n",
        "    if len(s.split(' ')) <= MAXLEN:\n",
        "        ctr_test += 1\n",
        "        filt_test_X.append(s.translate(str.maketrans('', '', string.punctuation)))\n",
        "        this_y = [0, 0]\n",
        "        this_y[label] = 1\n",
        "        filt_test_y.append(this_y)\n",
        "\n",
        "print(ctr_train, ctr_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Using Fatext Spanish embeddings\n",
        "\n",
        " We don't need the model itself. We just need to find where in the code embeddings are used and just add the Spanish embeddings there.\n",
        "\n",
        " We download the salient model from [https://github.com/dccuchile/spanish-word-embeddings?tab=readme-ov-file](https://github.com/dccuchile/spanish-word-embeddings?tab=readme-ov-file)\n",
        "\n",
        "Ex.: `embedding_model = ft.load_model(f'./dataset/cc.en.{MAXPARAMS}.bin')`"
      ],
      "metadata": {
        "id": "ny9-rf0smSZ9"
      },
      "id": "ny9-rf0smSZ9"
    },
    {
      "cell_type": "code",
      "source": [
        "#  download embeddings:\n",
        "# !wget https://zenodo.org/record/3234051/files/embeddings-l-model.bin?download=1"
      ],
      "metadata": {
        "id": "e0xzllz7Gl69",
        "outputId": "6140126a-552a-4733-d4db-3a1efe98e310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e0xzllz7Gl69",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-25 22:46:43--  https://zenodo.org/record/3234051/files/embeddings-l-model.bin?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.98.238, 188.184.103.159, 188.185.79.172, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.98.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/3234051/files/embeddings-l-model.bin [following]\n",
            "--2024-06-25 22:46:44--  https://zenodo.org/records/3234051/files/embeddings-l-model.bin\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5576446827 (5.2G) [application/octet-stream]\n",
            "Saving to: ‘embeddings-l-model.bin?download=1’\n",
            "\n",
            "embeddings-l-model. 100%[===================>]   5.19G  21.4MB/s    in 3m 32s  \n",
            "\n",
            "2024-06-25 22:50:16 (25.1 MB/s) - ‘embeddings-l-model.bin?download=1’ saved [5576446827/5576446827]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fasttext\n",
        "import fasttext as ft\n",
        "embedding_model = ft.load_model('/content/embeddings-l-model.bin?download=1')"
      ],
      "metadata": {
        "id": "9guWjHOlHeVZ"
      },
      "id": "9guWjHOlHeVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877d080a",
      "metadata": {
        "id": "877d080a"
      },
      "outputs": [],
      "source": [
        "from lambeq.text2diagram.tree_reader import BobcatParser\n",
        "import lambeq\n",
        "\n",
        "parser= BobcatParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4489dd44",
      "metadata": {
        "id": "4489dd44"
      },
      "outputs": [],
      "source": [
        "# Parse and create trees:\n",
        "train_diags = parser.sentences2diagrams(filt_train_X, suppress_exceptions=False)\n",
        "\n",
        "test_diags = parser.sentences2diagrams(filt_test_X, suppress_exceptions=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be9c058",
      "metadata": {
        "id": "0be9c058"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# We omit any case where the 2 phrases are not parsed to the same type\n",
        "# Khatri et al. is creating a circuit with the combination of X1 and X2.\n",
        "# We are not joining, so not needed\n",
        "# joint_diagrams_train = [d1 @ d2.r if d1.cod == d2.cod else None for d1 in train_diags]\n",
        "# joint_diagrams_test = [d1 @ d2.r if d1.cod == d2.cod else None for d1 in test_diags]\n",
        "\n",
        "#  Editing lines to get what we need from train_diags\n",
        "train_diags_raw = [d for d in train_diags if d is not None]\n",
        "train_y = np.array([y for d,y in zip(train_diags, filt_train_y) if d is not None])\n",
        "\n",
        "test_diags_raw = [d for d in test_diags if d is not None]\n",
        "test_y = np.array([y for d,y in zip(test_diags, filt_test_y) if d is not None])\n",
        "\n",
        "print(\"FINAL DATASET SIZE:\")\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"Training: Sentences- {len(train_diags_raw)} {Counter([tuple(elem) for elem in train_y])}\")\n",
        "print(f\"Testing : Sentences- {len(test_diags_raw)} {Counter([tuple(elem) for elem in test_y])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439fbd92",
      "metadata": {
        "id": "439fbd92"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from lambeq import Rewriter, RemoveCupsRewriter\n",
        "\n",
        "rewriter = RemoveCupsRewriter()\n",
        "# rewriter = Rewriter(['prepositional_phrase', 'determiner', 'coordination', 'connector', 'prepositional_phrase'])\n",
        "\n",
        "train_X = []\n",
        "test_X = []\n",
        "\n",
        "for d in tqdm(train_diags):\n",
        "    if d is not None:\n",
        "        train_X.append(rewriter(d).normal_form())\n",
        "    else:\n",
        "        print(\"found d is null\")\n",
        "\n",
        "for d in tqdm(test_diags):\n",
        "              if d is not None:\n",
        "                test_X.append(rewriter(d).normal_form())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d079fb",
      "metadata": {
        "id": "75d079fb"
      },
      "outputs": [],
      "source": [
        "# from discopy.quantum.gates import CX, Rx, H, Bra, Id\n",
        "\n",
        "# equality_comparator = (CX >> (H @ Rx(0.5)) >> (Bra(0) @ Id(1)))\n",
        "# equality_comparator.draw()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ToDo: find out what the following do:"
      ],
      "metadata": {
        "id": "r-UpMcaanjaQ"
      },
      "id": "r-UpMcaanjaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b80b7410",
      "metadata": {
        "id": "b80b7410"
      },
      "outputs": [],
      "source": [
        "from lambeq import AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz\n",
        "from lambeq import TketModel, NumpyModel, QuantumTrainer, SPSAOptimizer, Dataset\n",
        "import time\n",
        "import json\n",
        "\n",
        "SEED = 0\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 30\n",
        "\n",
        "N = AtomicType.NOUN\n",
        "S = AtomicType.SENTENCE\n",
        "P = AtomicType.PREPOSITIONAL_PHRASE\n",
        "\n",
        "\n",
        "def run_experiment(nlayers=1, seed=SEED):\n",
        "    print(f'RUNNING WITH {nlayers} layers and seed {seed}')\n",
        "    ansatz = Sim15Ansatz({N: 1, S: 1, P:1}, n_layers=nlayers, n_single_qubit_params=3)\n",
        "\n",
        "    train_circs = [ansatz(d) for d in train_X]\n",
        "    test_circs = [ansatz(d) for d in test_X]\n",
        "\n",
        "    lmbq_model = NumpyModel.from_diagrams(train_circs, use_jit=True)\n",
        "\n",
        "    trainer = QuantumTrainer(\n",
        "        lmbq_model,\n",
        "        loss_function=loss,\n",
        "        epochs=EPOCHS,\n",
        "        optimizer=SPSAOptimizer,\n",
        "        optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
        "        evaluate_functions=eval_metrics,\n",
        "        evaluate_on_train=True,\n",
        "        verbose = 'text',\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    train_dataset = Dataset(\n",
        "                train_circs,\n",
        "                train_y,\n",
        "                batch_size=BATCH_SIZE)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    train_embeddings, test_embeddings, max_w_param_length =\\\n",
        "      generate_initial_parameterisation(train_circs, test_circs, embedding_model, lmbq_model)\n",
        "\n",
        "    print('BEGINNING QNLP MODEL TRAINING')\n",
        "    trainer.fit(train_dataset)\n",
        "\n",
        "    train_preds = lmbq_model.get_diagram_output(train_circs)\n",
        "    train_loss = loss(train_preds, train_y)\n",
        "    train_acc = acc(train_preds, train_y)\n",
        "    print(f'TRAIN STATS: {train_loss, train_acc}')\n",
        "\n",
        "    print('BEGINNING DNN MODEL TRAINING')\n",
        "    NN_model = generate_OOV_parameterising_model(lmbq_model,\n",
        "                                                 train_embeddings,\n",
        "                                                 max_w_param_length)\n",
        "\n",
        "    prediction_model = NumpyModel.from_diagrams(test_circs, use_jit=True)\n",
        "\n",
        "    trained_wts = trained_params_from_model(lmbq_model, train_embeddings, max_w_param_length)\n",
        "\n",
        "    print('Evaluating SMART MODEL')\n",
        "    smart_loss, smart_acc = evaluate_test_set(prediction_model,\n",
        "                                              test_circs,\n",
        "                                              test_y,\n",
        "                                              trained_wts,\n",
        "                                              test_embeddings,\n",
        "                                              max_w_param_length,\n",
        "                                              OOV_strategy='model',\n",
        "                                              OOV_model=NN_model)\n",
        "\n",
        "    print('Evaluating EMBED model')\n",
        "    embed_loss, embed_acc = evaluate_test_set(prediction_model,\n",
        "                                              test_circs,\n",
        "                                              test_y,\n",
        "                                              trained_wts,\n",
        "                                              test_embeddings,\n",
        "                                              max_w_param_length,\n",
        "                                              OOV_strategy='embed')\n",
        "\n",
        "    print('Evaluating ZEROS model')\n",
        "    zero_loss, zero_acc = evaluate_test_set(prediction_model,\n",
        "                                              test_circs,\n",
        "                                              test_y,\n",
        "                                              trained_wts,\n",
        "                                              test_embeddings,\n",
        "                                              max_w_param_length,\n",
        "                                              OOV_strategy='zeros')\n",
        "\n",
        "    rand_losses = []\n",
        "    rand_accs = []\n",
        "\n",
        "    print('Evaluating RAND MODEL')\n",
        "    for _ in range(1000):\n",
        "\n",
        "\n",
        "        rl, ra = evaluate_test_set(prediction_model,\n",
        "                                   test_circs,\n",
        "                                   test_y,\n",
        "                                   trained_wts,\n",
        "                                   test_embeddings,\n",
        "                                   max_w_param_length,\n",
        "                                   OOV_strategy='random')\n",
        "\n",
        "        rand_losses.append(rl)\n",
        "        rand_accs.append(ra)\n",
        "\n",
        "    res =  {'TRAIN': (train_loss, train_acc),\n",
        "            'NN': (smart_loss, smart_acc),\n",
        "            'EMBED': (embed_loss, embed_acc),\n",
        "            'RAND': (rand_losses, rand_accs),\n",
        "            'ZERO': (zero_loss, zero_acc)\n",
        "           }\n",
        "    print(f'ZERO: {res[\"ZERO\"]}')\n",
        "    print(f'EMBED: {res[\"EMBED\"]}')\n",
        "    print(f'NN: {res[\"NN\"]}')\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "dd7385e7",
      "metadata": {
        "scrolled": true,
        "id": "dd7385e7",
        "outputId": "1267a06a-0ec8-4e31-acdf-2b53a6484a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING WITH 3 layers and seed 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'is_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f235b8f257a6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mthis_seed_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mthis_seed_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcompr_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf_seed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_seed_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-243081c6f6b8>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(nlayers, seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mansatz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSim15Ansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_single_qubit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_circs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_circs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-243081c6f6b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mansatz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSim15Ansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_single_qubit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_circs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_circs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mansatz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lambeq/ansatz/circuit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, diagram)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDiagram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCircuit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;34m\"\"\"Convert a lambeq diagram into a lambeq circuit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagram\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mob_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lambeq/backend/grammar.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, entity)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mob_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mob_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lambeq/backend/grammar.py\u001b[0m in \u001b[0;36mar_with_cache\u001b[0;34m(self, ar)\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_functor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_id'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "compr_results = {}\n",
        "\n",
        "tf_seeds = [0, 1, 2]\n",
        "\n",
        "for tf_seed in tf_seeds:\n",
        "    tf.random.set_seed(tf_seed)\n",
        "    this_seed_results = []\n",
        "    for nl in [3,2,1]:\n",
        "        this_seed_results.append(run_experiment(nl, tf_seed))\n",
        "    compr_results[tf_seed] = this_seed_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb6e2c0",
      "metadata": {
        "id": "8fb6e2c0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "bkup = compr_results\n",
        "\n",
        "with open('./results/MSR_OOV_S15.json', 'w') as f:\n",
        "    json.dump(bkup, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}