# -*- coding: utf-8 -*-
"""v7
#name: v7_*
# Code which takes uspantekan or spanish small data (100 sent) about two classes
education and dancing, runs it through a QNLP model, which is supported by a fasttext model,
and two neural network models to learn and make prediction.
#to get blow by blow details of what htis code does, refer to a section named
"how this code runs" inside the project plan
https://github.com/ua-datalab/QNLP/blob/main/Project-Plan.md
"""


import wandb
from tqdm import tqdm
# wandb.init(project="v4_uspantekan")
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import torch
from torch import nn
import wandb
import spacy
from lambeq import SpacyTokeniser
import numpy as np
import fasttext as ft
from lambeq import PytorchTrainer
from lightning.pytorch.loggers import WandbLogger
from lambeq.backend.tensor import Dim
from lambeq import AtomicType, SpiderAnsatz

ansatz_to_use = SpiderAnsatz
embedding_model = ft.load_model('./embeddings-l-model.bin')
#todo: find what maxparams is for
MAXPARAMS = 300


BATCH_SIZE = 30
EPOCHS = 2
LEARNING_RATE = 0.05
SEED = 0
DATA_BASE_FOLDER= "data"

USE_MRPC_DATA=False
USE_SPANISH_DATA=True
USE_USP_DATA=False

if(USE_USP_DATA):
    TRAIN="uspantek_train.txt"
    DEV="uspantek_dev.txt"
    TEST="uspantek_test.txt"

if(USE_SPANISH_DATA):
    TRAIN="spanish_train.txt"
    DEV="spanish_dev.txt"
    TEST="spanish_test.txt"


if(USE_MRPC_DATA):
    TRAIN="mrpc_train_80_sent.txt"
    DEV="mrpc_dev_10_sent.txt"
    TEST="mrpc_test_10sent.txt"

loss = lambda y_hat, y: -np.sum(y * np.log(y_hat)) / len(y)  # binary cross-entropy loss
acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting


spacy_tokeniser = SpacyTokeniser()

if(USE_SPANISH_DATA) or (USE_USP_DATA):
    spanish_tokeniser=spacy.load("es_core_news_sm")
    spacy_tokeniser.tokeniser = spanish_tokeniser



#for english tokenizer
if(USE_MRPC_DATA):
    english_tokenizer = spacy.load("en_core_web_sm")
    spacy_tokeniser.tokeniser =english_tokenizer


import os
from lambeq import BobcatParser

"""#set the the initial phases of the gates.
Also note that in this function is where we are walking into OOV land.
i.e we check if there are any words that are found only in test/val set
and not in train set.
mithuns comment @26thsep2024
"""
def generate_initial_parameterisation(train_circuits, test_circuits, embedding_model, qnlp_model):

    # extract the words from the circuits- i.e the training data
    # Note that in this vocab, the same word can have multiple types, which each occur separately
    # todo: what did he mean by same word having multiple types. 
    # is this the likes vs never example mentione din 1958 lambek paper?
    train_vocab = {symb.name.rsplit('_', 1)[0] for d in train_circuits for symb in d.free_symbols}
    test_vocab = {symb.name.rsplit('_', 1)[0] for d in test_circuits for symb in d.free_symbols}

    
    #todo print: the total number of words in train, and test+ note it down
    #.answer for spanish henderson there are 463 words in training and 89 in testing
    # out of the 89 words in test, 33 are not present in training, so they are OOV
    print(len(test_vocab.union(train_vocab)), len(train_vocab), len(test_vocab))    
    print(f"OOV word count: i.e out of {len(test_vocab)} there are  {len(test_vocab - train_vocab)} words that are not found in training. So they are OOV")

    #todo: find the meaning of symbol count- what is the difference between symbol count and OOV or train_vocab
    n_oov_symbs = len({symb.name for d in test_circuits for symb in d.free_symbols} - {symb.name for d in train_circuits for symb in d.free_symbols})
    print(f'OOV symbol count: {n_oov_symbs} / {len({symb.name for d in test_circuits for symb in d.free_symbols})}')

    """go through all thecircuits in training data, 
    and pick the one which has highest type value
    note that they are not using the literal length of the circuit, but
    the number attached to next to aldea_2...todo : find what exactly that does"""
    def get_max_word_param_length(input_circuits):
        lengths=[]
        for d in input_circuits:
            for symb in d.free_symbols:
                x =  symb.name.split('_', 1)[1]
                y = x.split('__')[0]
                lengths.append(int(y))
        return lengths
    
    max_word_param_length=0
    if(ansatz_to_use==SpiderAnsatz):
        max_word_param_length_train = max(get_max_word_param_length(train_circuits))
        max_word_param_length_test = max(get_max_word_param_length(test_circuits))
        max_word_param_length = max(max_word_param_length_train, max_word_param_length_test) + 1

    assert max_word_param_length!=0
    
    
    # max_word_param_length = max(, ) + 1
    print(f'Max params/word: {max_word_param_length}')

    """ # next , for each word in train and test vocab , we need to get its embedding from fasttext
    # mithuns comment @26thsep2024: 
    # note that there is some confusion between the input data Khatri used from MRPC
    # as oposed to the spanish one = rather how spider reader is storing it.
    # In MRPC and  bobcat parser, they store it in one for mat(i think its two underscore
    # while spider parser stores it with one underscore or two dashes or something
    its definitely a bug in their code. However, we bear the brunt since spider ansatz
    is the only one which didnt give errors for spanish data. So eventually this needs to be
    replaced/fixed/single format must be stored for all ansatz- evne see if you can
    create a pull request for this
    """
    if(ansatz_to_use==SpiderAnsatz):  
        # train_vocab_embeddings={}      
        def get_vocab_emb_dict(vocab):
            #i.e given a word from training and dev vocab, get the corresponding 
            # embedding using fast text. 
            #all this is stored as a key value pair in embed_dict, where the word is
            #the key and embedding is the value
            #todo: confirm if this is how Khatri does it too.
            embed_dict={}
            for wrd in vocab:
                """#spider ansatz alone writes the tokens in its vocabulary with a single underscore first and then a double underscore
                #so we need to parse accordingly
                #todo: some words are already in dictionary- i think this is because of the same
                #  words having multiple versions- mostly likely we shouldn't split the _1_ thing- i am thinking 
                #that denotes the nth TYPE of LIKES kinda adverbs."""
                cleaned_wrd=wrd.split('_')[0].replace('\\','').replace(",","")
                if cleaned_wrd in embed_dict   :
                    print(f"error.  the word {cleaned_wrd} was already in dict")
                else:
                    embed_dict[cleaned_wrd]= embedding_model[cleaned_wrd] 
            return embed_dict

            """ #for each word in train and test vocab get its embedding from fasttext
        #note that even though the symbols per se have _0_, in the train_vocab_embedding 
        # dictionary it is stored in the
        #format of {wrd: embedding}- i.e o
        # nly the word aldea out of aldea_0_ is separated out and used.
        # mithuns comment @26thsep2024: note that this is a hack, and ideally such data format
        # based difference shouldnt occur. 
        # TODO: ran khastri code on MRPC and confirm who is screwing up.
        # is it spider ansatz which is messing up the data format or is it us?
        # """
        train_vocab_embeddings = get_vocab_emb_dict(train_vocab)
        test_vocab_embeddings = get_vocab_emb_dict(test_vocab)
    
        

    else:
        #for the words created from other ansatz just write it as : _0_ so we can reuse the parsing from original khatri code. But here recording specifically for this instance
        train_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in train_vocab}
        test_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in test_vocab}


    #to store all the initial weights
    initial_param_vector = []

    #todo: find what qnlp_model.symbols is- rather how is it different than train vocab?
    for sym in qnlp_model.symbols:
        #@sep2nd2024-not sure what idx is supposed to do, am giong to give it the number associated with the word
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  sym.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = sym.name.split('_', 1)[1]
            idx = rest.split('__')[0]      
            #@sep2nd2024/ end of day: getting key error for lots of words - e.g. aldea..but why are words 
            # in qnlpmodel.symbols not getting the fasttext emb on the fly? why are we separating train_embeddings earlier?        
            #what is the meaning of symbols in qnlp.model
            #todo a) read the lambeq documentation on symbols
            #  b) read the 2010 discocat and CQM paper onwards up, chronologically
            #no point turning knobs without deeply understanding what symbols do
            #todo:compare the format ith mrpc data, and see if he is storing the initial param vector- and symbols with _0_ or not?
            if wrd in train_vocab_embeddings:
                initial_param_vector.append(train_vocab_embeddings[wrd][int(idx)])
            else:
                '''
                #todo: lots of words are getting hit with OOV- conirm why they are not there in fasttext emb
                # my guess is its all the unicode characters. 
                # In theory fast text is meant to create zero 
                # OOV..since it builds up from 1 gram 2 gram etc
                #update: this might be caused because am 
                # removing the _0_ thing from the actual name, without
                #  realizing what it is doing.
                
                found that this word verdad, was OOV/not in fasttext emb
                found that this word vió, was OOV/not in fasttext emb
                found that this word yo, was OOV/not in fasttext emb
                found that this word yyyyyy was OOV/not in fasttext emb
                '''
                print(f"found that this word {wrd} was OOV/not in fasttext emb")

    """
    Set the intialization of QNLP model's weight as that of the embeddings of each word
    Am not completely convinced about what he is doing here.
    FOr example in NN world, embedding is separate than weights of neurons.
    You might initialize the weights of neurons with random shit like Xavier glorot
    but almost nver initialize it with embedding itself.
    """
    qnlp_model.weights = nn.ParameterList(initial_param_vector)

    #note that he is not explicitly returning qnlp_model
    #todo- ensure that this qnlp_model is well defined in scope
    #this is bloody python- it will let all crap sneak through.
    return train_vocab_embeddings, test_vocab_embeddings, max_word_param_length

def trained_params_from_model(trained_qnlp_model, train_embeddings, max_word_param_length):

    """Read arguments from command line.

     Args:
    trained_qnlp_model- the trained_qnlp_model
    train_vocab_embeddings- the initial embeddings for words in the vocab got from fasttext
    max_word_param_length- what is the maximum size of a wor

    Returns:
        a map between each word and its latest final weights
    """

    trained_param_map = { symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}
    trained_parameterisation_map = {wrd: np.zeros(max_word_param_length) for wrd in train_embeddings}

    for symbol, train_val in trained_param_map.items():
        
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  symbol.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = symbol.name.split('_', 1)[1]
            idx = rest.split('__')[0]    
        else:
            wrd, idx = symbol.name.rsplit('_', 1)
        
        #this is a hack i.e searching for wrd. Ideally it should be there. (aldea- was equivalent
        if wrd in trained_parameterisation_map:
            trained_parameterisation_map[wrd][int(idx)] = train_val

    return trained_parameterisation_map
def generate_OOV_parameterising_model(trained_qnlp_model, train_vocab_embeddings, max_word_param_length):
    """Read arguments from command line.

    Args:
    trained_qnlp_model- the trained_qnlp_model
    train_vocab_embeddings- the initial embeddings for words in the vocab got from fasttext
    max_word_param_length- what is the maximum size of a word

    Returns:
    Weights of a NN model which now understands/has weights for each word in fasttext as its original embedding influenced/mapped to the weights in the trained QNLP model

    """

    
    trained_params_raw = {symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}

    """explanation of above code:
    trained_params_raw is dictionary that map words in the trained QNLP model to 
    its weights at the end of QNLP training. note that currently/initially the weights are 
    the initial fast text embedding values- we are just initializing it using a standard set of 
    values /embeddings-which will then be updated as the multi layer perceptron learns the connection
    or mapping or relationship between fast text embeddings, and the angles learned by the QNLP model
    
    #todo: print and confirm if symbol means word
    #update@sep 11th 2024 : symbol is not word, it is word+ that idx number- 
    # which i am suspecting is the number of Types a same word can have
    # for example if you read 1958 Lambek paper youc an see that adverb Likes can have two different TYPE representations.
    # but yes, todo: confirm this by comparing it with the original Khatri MRPC code."""


    print(trained_params_raw)
    
    
    trained_param_vectors = {wrd: np.zeros(max_word_param_length) for wrd in train_vocab_embeddings}
    """
    # train_vocab_embeddings are the initial embeddings 
    for words in the vocab we got from fasttext- for each such word create an array of zeroes
    - this array is where the weights of the MLP will be added, i think?

    #i.e for now for each such word in training vocabulary, create a vector filled with zeroes to represent
    #trained parameters (todo: what exaclty is trained parameters- is it same as weights of the MLP 
    #neural network?) whose size will be the max size of sentence in training
    #todo: is this being done for padding?

    """
    print(trained_param_vectors)
    
    '''for each such symbol and parameter weight assign the weights
      to the empty array/array of zeroes   created above.-
    #todo 1) am still not sure why he is touching his nose in a circumambulated way
    # 2)  print and confirm, i think they are repeating the same
    #  weight value for every entry of the array in trained_param_vectors'''
    for symbol, train_val in trained_params_raw.items():
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  symbol.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = symbol.name.split('_', 1)[1]
            idx = rest.split('__')[0]    
        else:
            wrd, idx = symbol.name.rsplit('_', 1)
        
        if wrd in trained_param_vectors:
                trained_param_vectors[wrd][int(idx)] = train_val
        else:                
                print(f"inside OOV_generation-found that this word {wrd} was not in trained_param_vectors")

                

    wrds_in_order = list(train_vocab_embeddings.keys())

    #so the value to be trained now are the initial weights of each word from
    #fasttext, which will be trained against  gold label -trained_param_vectors i.e weights from the trained QNLP model
    NN_train_X = np.array([train_vocab_embeddings[wrd] for wrd in wrds_in_order])
    NN_train_Y = np.array([trained_param_vectors[wrd] for wrd in wrds_in_order])

    print(NN_train_X[0][:5])
    print(NN_train_Y[0][:5])

    

    OOV_NN_model = keras.Sequential([
      layers.Dense(int((max_word_param_length + MAXPARAMS) / 2), activation='tanh'),
      layers.Dense(max_word_param_length, activation='tanh'),
    ])

    OOV_NN_model.compile(loss='mean_absolute_error', optimizer=keras.optimizers.Adam(0.001))

    # Embedding dim!
    OOV_NN_model.build(input_shape=(None, MAXPARAMS))

    hist = OOV_NN_model.fit(NN_train_X, NN_train_Y, validation_split=0.2, verbose=0, epochs=120)

    print(f'OOV NN model final epoch loss: {(hist.history["loss"][-1], hist.history["val_loss"][-1])}')

    plt.plot(hist.history['loss'], label='loss')
    plt.plot(hist.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.show()

    return OOV_NN_model
def evaluate_test_set(pred_model, test_circuits, test_labels, trained_params, test_vocab_embeddings, max_word_param_length, OOV_strategy='random', OOV_model=None):

    pred_parameter_map = {}

    # Use the words from train wherever possible, else use DNN prediction
    for wrd, embedding in test_vocab_embeddings.items():
        if OOV_strategy == 'model':
            pred_parameter_map[wrd] = trained_params.get(wrd, OOV_model.predict(np.array([embedding]), verbose=0)[0])
        elif OOV_strategy == 'embed':
            pred_parameter_map[wrd] = trained_params.get(wrd, embedding)
        elif OOV_strategy == 'zeros':
            pred_parameter_map[wrd] = trained_params.get(wrd, np.zeros(max_word_param_length))
        else:
            pred_parameter_map[wrd] = trained_params.get(wrd, 2 * np.random.rand(max_word_param_length)-1)

    pred_weight_vector = []

    for sym in pred_model.symbols:
        wrd, idx = sym.name.rsplit('_', 1)
        pred_weight_vector.append(pred_parameter_map[wrd][int(idx)])

    pred_model.weights = pred_weight_vector

    preds = pred_model.get_diagram_output(test_circuits)

    return loss(preds, test_labels), acc(preds, test_labels)

def read_data(filename):
    labels, sentences = [], []
    with open(filename) as f:
        for line in f:
            t = float(line[0])
            labels.append([t, 1-t])
            sentences.append(line[1:].strip())
    return labels, sentences

#back to the main thread after all functions are defined.

#read the base data, i.e plain text english.
train_labels, train_data = read_data(os.path.join(DATA_BASE_FOLDER,TRAIN))
val_labels, val_data = read_data(os.path.join(DATA_BASE_FOLDER,DEV))
test_labels, test_data = read_data(os.path.join(DATA_BASE_FOLDER,TEST))

#setting a flag for TESTING so that it is done only once.
#  Everything else is done on train and dev
TESTING = int(os.environ.get('TEST_NOTEBOOKS', '0'))

if TESTING:
    train_labels, train_data = train_labels[:2], train_data[:2]
    val_labels, val_data = val_labels[:2], val_data[:2]
    test_labels, test_data = test_labels[:2], test_data[:2]
    EPOCHS = 1


"""
# not using bob cat parser- note: this wasn't compatible with spider ansatz

we are using spiders reader for this code, because pytorch trainer goes well
#with it. Note that this is being done in SEp 2024 to just get the
#  code to take off from the ground. However, other than this being a good baseline, 
spider reader should be soon discareded and switched to bob cat parser+ some
quantum traineres.



# """

from lambeq import spiders_reader
#parser = BobcatParser(verbose='text')

"""spanish_diagrams is a dummy function I had created once when I had to test
both MRPC and uspantekan/spanish data at the same time. it is very useful in debugging
especially for converting aldea_0 kinda format issues. But do remove this
once not needed. Mithun@26th sep 2024"""
def spanish_diagrams(list_sents,labels):
    list_target = []
    labels_target = []
    for sent, label in tqdm(zip(list_sents, labels),desc="reading sent"):
        
        # tokenized = spacy_spanish_tokeniser.tokenise_sentence(sent)
        # diag =parser.sentence2diagram(tokenized, tokenised= True)
        # diag.draw()
        # list_target.append(diag)
        
        if( USE_MRPC_DATA):
            sent = sent.split('\t')[2]
        tokenized = spacy_tokeniser.tokenise_sentence(sent)
        
        
        if(not USE_MRPC_DATA):
            if len(tokenized)> 30:
                print(f"no of tokens inthis sentence is {len(tokenized)}")
                continue
        spiders_diagram = spiders_reader.sentence2diagram(sent)
        list_target.append(spiders_diagram)
        labels_target.append(label)
    print("no. of items processed= ", len(list_target))
    return list_target, labels_target

#convert the plain text input to ZX diagrams.
train_diagrams, train_labels_v2 = spanish_diagrams(train_data,train_labels)
val_diagrams, val_labels_v2 = spanish_diagrams(val_data,val_labels)
test_diagrams, test_labels_v2 = spanish_diagrams(test_data,test_labels)

train_labels = train_labels_v2
val_labels = val_labels_v2
test_labels = test_labels_v2



"""
print and assert statements for debugging
"""
assert len(train_diagrams)== len(train_labels_v2)
print(len(train_diagrams), len(test_diagrams), len(val_diagrams))
assert len(train_diagrams)== len(train_labels)
assert len(val_diagrams)== len(val_labels)
assert len(test_diagrams)== len(test_labels)



"""mithuns comment @26thsep2024typically spider ansatz only goes with spider reader. 
like i mentioned earlier, spider was used to just get the code off the ground
1) we need to definitely test with atleast bobcat parser
2) Noun should have a higher dimension than sentence? how? 
- go back and confirm the original 1958 paper by lambek. also how
 is the code in LAMBEQ deciding the dimensions or even what  data types to use?
 answer might be in 2010 discocat paper"""
ansatz = ansatz_to_use({AtomicType.NOUN: Dim(4),
                       AtomicType.SENTENCE: Dim(2)
                    #    AtomicType.PREPOSITIONAL_PHRASE: Dim(2),
                       })

#use the anstax to create circuits from diagrams
train_circuits =  [ansatz(diagram) for diagram in train_diagrams]
val_circuits =  [ansatz(diagram) for diagram in val_diagrams]
test_circuits = [ansatz(diagram) for diagram in test_diagrams]


from lambeq import PytorchModel

#mithuns comment @26thsep2024:pytorch model was the only one going well with spider reader and spider anstaz
#also todo: should this not contain both val and train circuits as inputz?
qnlp_model = PytorchModel.from_diagrams(train_circuits)

sig = torch.sigmoid

def accuracy(y_hat, y):
    return torch.sum(torch.eq(torch.round(sig(y_hat)), y))/len(y)/2  # half due to double-counting

eval_metrics = {"acc": accuracy}

from lambeq import Dataset

print([len(x) for x in train_circuits])

train_dataset = Dataset(
            train_circuits,
            train_labels,
            batch_size=BATCH_SIZE)

val_dataset = Dataset(val_circuits, val_labels, shuffle=False)

print(len(train_labels), len(train_circuits))
#print and assert statements for debugging
print(len(train_circuits), len(val_circuits), len(test_circuits))
assert len(train_circuits)== len(train_labels)
assert len(val_circuits)== len(val_labels)
assert len(test_circuits)== len(test_labels)

sweep_config = {
    'method': 'random'
    }
metric = {
    'name': 'loss',
    'goal': 'minimize'
    }

sweep_config['metric'] = metric

parameters_dict = {
    'LEARNING_RATE': {
          'values': [0.3, 0.03, 0.003,0.0003]
        },
    }

sweep_config['parameters'] = parameters_dict

parameters_dict.update({
    'epochs': {
        'value': 1}
    })

import pprint
#this is wandb crap,comment out if becoming a pain in the butt. MLFlow is better than wandb imho
pprint.pprint(sweep_config)
# sweep_id = wandb.sweep(sweep_config, project="uspantekan_spider_tuning")
# wandb_logger = WandbLogger()

trainer = PytorchTrainer(
        model=qnlp_model,
        loss_function=torch.nn.BCEWithLogitsLoss(),
        optimizer=torch.optim.AdamW,
        learning_rate=LEARNING_RATE,
        epochs=EPOCHS,
        evaluate_functions=eval_metrics,
        evaluate_on_train=True,
        verbose='text',
        seed=SEED)



# train_preds = qnlp_model.get_diagram_output(train_circuits)
# loss_pyTorch = nn.CrossEntropyLoss()
# loss_pyTorch(train_preds, train_labels)
# print(f"TRAIN STATS: {loss_pyTorch}")

# #todo add f1 also

# train_loss = loss(train_preds, train_labels)
# train_acc = acc(train_preds, train_labels)
# print(f'TRAIN STATS: {train_loss, train_acc}')

"""by here the actual QNLP model is trained. Next is we are going to connect the model which learns
#  relationsihp between fasttext emb and angles. look inside the function
generate_initial_parameterisation for more specific comments

"""

train_embeddings, test_embeddings, max_w_param_length = generate_initial_parameterisation(train_circuits, val_circuits, embedding_model, qnlp_model)

#run ONLY the QNLP model.i.e let it train on the train_dataset. and test on val_dataset
#todo: bloody somewhere you use val and somehwere you use test. Fix it/use only one everywhere

trainer.fit(train_dataset, val_dataset, eval_interval=1, log_interval=1)


#now we get into the land of model 3
# print('BEGINNING DNN MODEL TRAINING')
NN_model = generate_OOV_parameterising_model(qnlp_model, train_embeddings, max_w_param_length)
prediction_model = PytorchModel.from_diagrams(test_circuits)
trained_wts = trained_params_from_model(qnlp_model, train_embeddings, max_w_param_length)

smart_loss, smart_acc = evaluate_test_set(prediction_model,
                                              test_circuits,
                                              test_labels,
                                              trained_wts,
                                              test_embeddings,
                                              max_w_param_length,
                                              OOV_strategy='model',
                                              OOV_model=NN_model)


