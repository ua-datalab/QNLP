# -*- coding: utf-8 -*-
"""v7
#name: v7_*
# Code which takes uspantekan or spanish small data (100 sent) about two classes
education and dancing, runs it through a QNLP model, which is supported by a fasttext model,
and two neural network models to learn and make prediction.
#to get blow by blow details of what htis code does, refer to a section named
"how this code runs" inside the project plan
https://github.com/ua-datalab/QNLP/blob/main/Project-Plan.md

4 major models used in this code. 
1. QNLP model, called model1
2. Fast text embedding model , called model 2
3. NN model that learns mapping between fast text embedding and QNLP trained model's weights
4. Prediction model - which is use ddto predict on test set.
#todo, find why not just do model1.predict?
"""


import wandb
from tqdm import tqdm
# wandb.init(project="v4_uspantekan")
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import torch
from torch import nn
import wandb
import spacy
from lambeq import SpacyTokeniser
import numpy as np
import fasttext as ft
from lambeq import PytorchTrainer
from lightning.pytorch.loggers import WandbLogger
from lambeq.backend.tensor import Dim
from lambeq import AtomicType, SpiderAnsatz
from lambeq import Dataset
ansatz_to_use = SpiderAnsatz
embedding_model = ft.load_model('./embeddings-l-model.bin')
#todo: find what maxparams is for
MAXPARAMS = 300


BATCH_SIZE = 30
EPOCHS = 2
LEARNING_RATE = 0.05
SEED = 0
DATA_BASE_FOLDER= "data"

USE_MRPC_DATA=False
USE_SPANISH_DATA=True
USE_USP_DATA=False

if(USE_USP_DATA):
    TRAIN="uspantek_train.txt"
    DEV="uspantek_dev.txt"
    TEST="uspantek_test.txt"

if(USE_SPANISH_DATA):
    TRAIN="spanish_train.txt"
    DEV="spanish_dev.txt"
    TEST="spanish_test.txt"


if(USE_MRPC_DATA):
    TRAIN="mrpc_train_80_sent.txt"
    DEV="mrpc_dev_10_sent.txt"
    TEST="mrpc_test_10sent.txt"

loss = lambda y_hat, y: -np.sum(y * np.log(y_hat)) / len(y)  # binary cross-entropy loss
acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting
sig = torch.sigmoid

def accuracy(y_hat, y):
        return torch.sum(torch.eq(torch.round(sig(y_hat)), y))/len(y)/2  # half due to double-counting

eval_metrics = {"acc": accuracy}
spacy_tokeniser = SpacyTokeniser()

if(USE_SPANISH_DATA) or (USE_USP_DATA):
    spanish_tokeniser=spacy.load("es_core_news_sm")
    spacy_tokeniser.tokeniser = spanish_tokeniser



#for english tokenizer
if(USE_MRPC_DATA):
    english_tokenizer = spacy.load("en_core_web_sm")
    spacy_tokeniser.tokeniser =english_tokenizer


import os
from lambeq import BobcatParser

"""go through all thecircuits in training data, 
    and pick the one which has highest type value
    note that they are not using the literal length of the circuit, but
    the number attached to next to aldea_2...todo : find what exactly that does"""
def get_max_word_param_length(input_circuits):
        lengths=[]
        for d in input_circuits:
            for symb in d.free_symbols:
                x =  symb.name.split('_', 1)[1]
                y = x.split('__')[0]
                lengths.append(int(y))
        return lengths

def get_vocab_emb_dict(vocab):
            #i.e given a word from training and dev vocab, get the corresponding 
            # embedding using fast text. 
            #all this is stored as a key value pair in embed_dict, where the word is
            #the key and embedding is the value
            #todo: confirm if this is how Khatri does it too.
            embed_dict={}
            for wrd in vocab:
                """#spider ansatz alone writes the tokens in its vocabulary with a single underscore first and then a double underscore
                #so we need to parse accordingly
                #todo: some words are already in dictionary- i think this is because of the same
                #  words having multiple versions- mostly likely we shouldn't split the _1_ thing- i am thinking 
                #that denotes the nth TYPE of LIKES kinda adverbs."""
                cleaned_wrd=wrd.split('_')[0].replace('\\','').replace(",","")
                if cleaned_wrd in embed_dict   :
                    print(f"error.  the word {cleaned_wrd} was already in dict")
                else:
                    embed_dict[cleaned_wrd]= embedding_model[cleaned_wrd] 
            return embed_dict

"""#set the the initial phases of the gates.
Also note that in this function is where we are walking into OOV land.
i.e we check if there are any words that are found only in test/val set
and not in train set.
mithuns comment @26thsep2024
"""
def generate_initial_parameterisation(train_circuits, test_circuits, embedding_model, qnlp_model):

    # extract the words from the circuits- i.e the training data
    # Note that in this vocab, the same word can have multiple types, which each occur separately
    # todo: what did he mean by same word having multiple types. 
    # is this the likes vs never example mentione din 1958 lambek paper?
    train_vocab = {symb.name.rsplit('_', 1)[0] for d in train_circuits for symb in d.free_symbols}
    test_vocab = {symb.name.rsplit('_', 1)[0] for d in test_circuits for symb in d.free_symbols}

    
    #todo print: the total number of words in train, and test+ note it down
    #.answer for spanish henderson there are 463 words in training and 89 in testing
    # out of the 89 words in test, 33 are not present in training, so they are OOV
    print(len(test_vocab.union(train_vocab)), len(train_vocab), len(test_vocab))    
    print(f"OOV word count: i.e out of {len(test_vocab)} there are  {len(test_vocab - train_vocab)} words that are not found in training. So they are OOV")

    #todo: find the meaning of symbol count- what is the difference between symbol count and OOV or train_vocab
    n_oov_symbs = len({symb.name for d in test_circuits for symb in d.free_symbols} - {symb.name for d in train_circuits for symb in d.free_symbols})
    print(f'OOV symbol count: {n_oov_symbs} / {len({symb.name for d in test_circuits for symb in d.free_symbols})}')

    
    
    max_word_param_length=0
    if(ansatz_to_use==SpiderAnsatz):
        max_word_param_length_train = max(get_max_word_param_length(train_circuits))
        max_word_param_length_test = max(get_max_word_param_length(test_circuits))
        max_word_param_length = max(max_word_param_length_train, max_word_param_length_test) + 1

    assert max_word_param_length!=0

    
    # max_word_param_length = max(, ) + 1
    print(f'Max params/word: {max_word_param_length}')

    """ # next , for each word in train and test vocab , we need to get its embedding from fasttext
    # mithuns comment @26thsep2024: 
    # note that there is some confusion between the input data Khatri used from MRPC
    # as oposed to the spanish one = rather how spider reader is storing it.
    # In MRPC and  bobcat parser, they store it in one for mat(i think its two underscore
    # while spider parser stores it with one underscore or two dashes or something
    its definitely a bug in their code. However, we bear the brunt since spider ansatz
    is the only one which didnt give errors for spanish data. So eventually this needs to be
    replaced/fixed/single format must be stored for all ansatz- evne see if you can
    create a pull request for this
    """
    if(ansatz_to_use==SpiderAnsatz):  
        # train_vocab_embeddings={}      
        

        """ #for each word in train and test vocab get its embedding from fasttext
        #note that even though the symbols per se have _0_, in the train_vocab_embedding 
        # dictionary it is stored in the
        #format of {wrd: embedding}- i.e o
        # nly the word aldea out of aldea_0_ is separated out and used.
        # mithuns comment @26thsep2024: note that this is a hack, and ideally such data format
        # based difference shouldnt occur. 
        # TODO: ran khastri code on MRPC and confirm who is screwing up.
        # is it spider ansatz which is messing up the data format or is it us?
        # """
        train_vocab_embeddings = get_vocab_emb_dict(train_vocab)
        test_vocab_embeddings = get_vocab_emb_dict(test_vocab)
    
        

    else:
        #for the words created from other ansatz just write it as : _0_ so we can reuse the parsing from original khatri code. But here recording specifically for this instance
        train_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in train_vocab}
        test_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in test_vocab}


    #to store all the initial weights
    initial_param_vector = []

    #todo: find what qnlp_model.symbols is- rather how is it different than train vocab?
    for sym in qnlp_model.symbols:
        #@sep2nd2024-not sure what idx is supposed to do, am giong to give it the number associated with the word
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  sym.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = sym.name.split('_', 1)[1]
            idx = rest.split('__')[0]      
            #@sep2nd2024/ end of day: getting key error for lots of words - e.g. aldea..but why are words 
            # in qnlpmodel.symbols not getting the fasttext emb on the fly? why are we separating train_embeddings earlier?        
            #what is the meaning of symbols in qnlp.model
            #todo a) read the lambeq documentation on symbols
            #  b) read the 2010 discocat and CQM paper onwards up, chronologically
            #no point turning knobs without deeply understanding what symbols do
            #todo:compare the format ith mrpc data, and see if he is storing the initial param vector- and symbols with _0_ or not?
            if wrd in train_vocab_embeddings:
                initial_param_vector.append(train_vocab_embeddings[wrd][int(idx)])
            else:
                '''
                #todo: lots of words are getting hit with OOV- conirm why they are not there in fasttext emb
                # my guess is its all the unicode characters. 
                # In theory fast text is meant to create zero 
                # OOV..since it builds up from 1 gram 2 gram etc
                #update: this might be caused because am 
                # removing the _0_ thing from the actual name, without
                #  realizing what it is doing.
                
                found that this word verdad, was OOV/not in fasttext emb
                found that this word viÃ³, was OOV/not in fasttext emb
                found that this word yo, was OOV/not in fasttext emb
                found that this word yyyyyy was OOV/not in fasttext emb
                '''
                print(f"found that this word {wrd} was OOV/not in fasttext emb")

    """
    Set the intialization of QNLP model's weight as that of the embeddings of each word
    Am not completely convinced about what he is doing here.
    FOr example in NN world, embedding is separate than weights of neurons.
    You might initialize the weights of neurons with random shit like Xavier glorot
    but almost nver initialize it with embedding itself.
    """
    qnlp_model.weights = nn.ParameterList(initial_param_vector)

    #note that he is not explicitly returning qnlp_model
    #todo- ensure that this qnlp_model is well defined in scope
    #this is bloody python- it will let all crap sneak through.
    return train_vocab_embeddings, test_vocab_embeddings, max_word_param_length

def trained_params_from_model(trained_qnlp_model, train_embeddings, max_word_param_length):

    """

     Args:
    trained_qnlp_model- the trained_qnlp_model
    train_vocab_embeddings- the initial embeddings for words in the vocab got from fasttext
    max_word_param_length- what is the maximum size of a wor

    Returns:
        a map between each word and its latest final weights
    """

    trained_param_map = { symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}
    trained_parameterisation_map = {wrd: np.zeros(max_word_param_length) for wrd in train_embeddings}

    for symbol, train_val in trained_param_map.items():
        
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  symbol.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = symbol.name.split('_', 1)[1]
            idx = rest.split('__')[0]    
        else:
            wrd, idx = symbol.name.rsplit('_', 1)
        
        #this is a hack i.e searching for wrd. Ideally it should be there. (aldea- was equivalent
        if wrd in trained_parameterisation_map:
            trained_parameterisation_map[wrd][int(idx)] = train_val

    return trained_parameterisation_map

def generate_OOV_parameterising_model(trained_qnlp_model, train_vocab_embeddings, max_word_param_length):
    """
    in the previous func `generate_initial_parameterisation` we took model 1 i.e the QNLP model
    and initialized its weights with the embeddings of the words

    here we will take the model 1, and create another NN i.e model 3, which will learn the mapping between
    train_vocab_embeddings and weights of trained_qnlp_model
    Args:
    trained_qnlp_model- the trained_qnlp_model
    train_vocab_embeddings- the initial embeddings for words in the vocab got from fasttext
    max_word_param_length- what is the maximum size of a word

    Returns:
    Weights of a NN model which now has learnt
      for each word in fasttext as its original embedding mapped to the weights in the trained QNLP model

    """


    """explanation of dict_symbols_vs_qnlp_trained_weights:
    dict_symbols_vs_qnlp_trained_weights is a dictionary that map words in the trained QNLP model to 
    its weights at the end of QNLP training i.e the training that happened to model 1 i.e the QNLP model
    
    #todo: print and confirm if symbol means word
    #update@sep 11th 2024 : symbol is not word, it is word+ that idx number- 
    # which i am suspecting is the number of Types a same word can have
    # for example if you read 1958 Lambek paper youc an see that adverb Likes can have two different TYPE representations.
    # but yes, todo: confirm this by comparing it with the original Khatri MRPC code.
    # todo: he is doing the same process above inside the function generate_initial_parameterisation
    around line 222. Find out why doing it so many times? are they same? so why not just pass around?
    # """

    
    dict_training_symbols_vs_qnlp_trained_weights = {symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}
   
   
    """
    train_vocab_embeddings are the initial embeddings 
    for words in the vocab we got from fasttext-     
    now for each such word create an array of zeroes called trained_param_vectors
    - this array is where the weights of the model 3, NN model which maps between embedding and learned weights of 
    QNLP model will be added.
    i.e for now for each such word in training vocabulary, create a vector filled with zeroes to represent
    trained parameters         
    """
    dict_wrd_in_training_vs_weights = {wrd: np.zeros(max_word_param_length) for wrd in train_vocab_embeddings}
    
    '''for each such word in training vocabulary, 
      to the empty array/array of zeroes   created above.-
    #todo 1) am still not sure why he is touching his nose in a circumambulated way
    # 2)  print and confirm, i think they are repeating the same
    #  weight value for every entry of the array in trained_param_vectors.
    # 
    # Note that this for loop below is purely done so as to extract word out of symbol.
    # the dictionary intuition remains same between both dictionaries. i.e dict_training_symbols_vs_qnlp_trained_weights and 
    # dict_wrd_in_training_vs_weights i.e key is either word/symbol while value is sambe for both, which is the corresponding 
    # weight of the same word in the trained QNLP model '''
    
    for symbol, trained_weights in dict_training_symbols_vs_qnlp_trained_weights.items():
        if(ansatz_to_use==SpiderAnsatz):  
            #symbol and word are different. e.g. aldea_0. From this extract the word
            wrd =  symbol.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = symbol.name.split('_', 1)[1]
            idx = rest.split('__')[0]    
        else:
            wrd, idx = symbol.name.rsplit('_', 1)
        
        #if that word is in train vocab (from the embedding side)- 99% should be there.
        #todo: confirm if any words are outside
        if wrd in dict_wrd_in_training_vs_weights:
                dict_wrd_in_training_vs_weights[wrd][int(idx)] = trained_weights
        else:                
                print(f"inside OOV_generation-found that this word {wrd} was not in trained_param_vectors")

                

    wrds_in_order = list(train_vocab_embeddings.keys())

    """#For each word in a ordered list of training vocabulary words, create 2 arrays. 
    One for embeddings (NN_train_X) and the other for trained weights (NN_train_Y)
    Note, the goal of model 3 is to learn the mapping between these two things"""
    NN_train_X = np.array([train_vocab_embeddings[wrd] for wrd in wrds_in_order])
    NN_train_Y = np.array([dict_wrd_in_training_vs_weights[wrd] for wrd in wrds_in_order])
    
    #this is model 3. i.e create a simple Keras NN model, which will learn the above mapping
    OOV_NN_model = keras.Sequential([
      layers.Dense(int((max_word_param_length + MAXPARAMS) / 2), activation='tanh'),
      layers.Dense(max_word_param_length, activation='tanh'),
    ])

    #standard keras stuff, initialize and tell what loss function and which optimizer will you be using
    OOV_NN_model.compile(loss='mean_absolute_error', optimizer=keras.optimizers.Adam(0.001))

    # Embedding dim!
    """#todo find why maxparams are hardcoded as 300 (is it the dimension of the fasttext embedding?)
    #ans: yes. The first layer needs to have the dimension of NN_train_X, which in turn has the dimension of the  
    Fasttext embedding"""
    OOV_NN_model.build(input_shape=(None, MAXPARAMS))

    #train that model 3
    hist = OOV_NN_model.fit(NN_train_X, NN_train_Y, validation_split=0.2, verbose=0, epochs=120)

    print(f'OOV NN model final epoch loss: {(hist.history["loss"][-1], hist.history["val_loss"][-1])}')

    plt.plot(hist.history['loss'], label='loss')
    plt.plot(hist.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.show()

    return OOV_NN_model

def evaluate_test_set(pred_model, test_circuits, test_labels, trained_params, test_vocab_embeddings, max_word_param_length, OOV_strategy='random', OOV_model=None):
    """
    So this is where we do the final testing (even if its over dev)
    Here they are sending 
    a) pred_model:the newly created prediction model - which is same as the original QNLP model fundamentally.
    b) test circuits, test_labels- what it says
    c) trained_params: trained_wts from the model 1- qnlp model (my guess is he is going to assign these weights to the 
    newly created model. Though why he is doing it in a circumambulated way while he could have directly 
    used qnlp_model is beyond me)
    d) test_vocab_embeddings: Take every word in test/val set, give it to fasttext, and get their embeddings
    e) max_word_param_length: refer function get_max_word_param_length
    f) oov_strategy= picking one hardcoded one from [zero,embed, model,random etc]- basically this is where you 
    decide what baseline model do you want to use for your model 3- rather, do you want to use any model at all
    or do you want to use things like baselines methods like random number generator, fill with zeroes etc.

    """


    pred_parameter_map = {}
    #Use the words from train wherever possible, else use DNN prediction
    for wrd, embedding in test_vocab_embeddings.items():
        if OOV_strategy == 'model':
            """
            for each word in test/dev vocabulary, give the word as input to model 3 (the NN model also called DNN here)
            which now will ideally return the corresponding weight of the QNLP trained model- because model 3 had learned
            the mapping between these two things. REfer: generate_oov_* function above.
            Note that all other strategies below are time pass/ boring hard coded baseline stuff
            """
            pred_parameter_map[wrd] = trained_params.get(wrd, OOV_model.predict(np.array([embedding]), verbose=0)[0])
        elif OOV_strategy == 'embed':
            pred_parameter_map[wrd] = trained_params.get(wrd, embedding)
        elif OOV_strategy == 'zeros':
            pred_parameter_map[wrd] = trained_params.get(wrd, np.zeros(max_word_param_length))
        else:
            pred_parameter_map[wrd] = trained_params.get(wrd, 2 * np.random.rand(max_word_param_length)-1)

    pred_weight_vector = []

    for sym in pred_model.symbols:
        wrd, idx = sym.name.rsplit('_', 1)
        pred_weight_vector.append(pred_parameter_map[wrd][int(idx)])

    pred_model.weights = pred_weight_vector

    preds = pred_model.get_diagram_output(test_circuits)

    return loss(preds, test_labels), acc(preds, test_labels)

def read_data(filename):
    labels, sentences = [], []
    with open(filename) as f:
        for line in f:
            t = float(line[0])
            labels.append([t, 1-t])
            sentences.append(line[1:].strip())
    return labels, sentences

#back to the main thread after all functions are defined.

#read the base data, i.e plain text english.
train_labels, train_data = read_data(os.path.join(DATA_BASE_FOLDER,TRAIN))
val_labels, val_data = read_data(os.path.join(DATA_BASE_FOLDER,DEV))
test_labels, test_data = read_data(os.path.join(DATA_BASE_FOLDER,TEST))

#setting a flag for TESTING so that it is done only once.
#  Everything else is done on train and dev
TESTING = int(os.environ.get('TEST_NOTEBOOKS', '0'))

if TESTING:
    train_labels, train_data = train_labels[:2], train_data[:2]
    val_labels, val_data = val_labels[:2], val_data[:2]
    test_labels, test_data = test_labels[:2], test_data[:2]
    EPOCHS = 1


"""
# not using bob cat parser- note: this wasn't compatible with spider ansatz

we are using spiders reader for this code, because pytorch trainer goes well
#with it. Note that this is being done in SEp 2024 to just get the
#  code to take off from the ground. However, other than this being a good baseline, 
spider reader should be soon discareded and switched to bob cat parser+ some
quantum traineres.



# """

from lambeq import spiders_reader
#parser = BobcatParser(verbose='text')

"""spanish_diagrams is a dummy function I had created once when I had to test
both MRPC and uspantekan/spanish data at the same time. it is very useful in debugging
especially for converting aldea_0 kinda format issues. But do remove this
once not needed. Mithun@26th sep 2024"""
def spanish_diagrams(list_sents,labels):
    list_target = []
    labels_target = []
    for sent, label in tqdm(zip(list_sents, labels),desc="reading sent"):
        
        # tokenized = spacy_spanish_tokeniser.tokenise_sentence(sent)
        # diag =parser.sentence2diagram(tokenized, tokenised= True)
        # diag.draw()
        # list_target.append(diag)
        
        if( USE_MRPC_DATA):
            sent = sent.split('\t')[2]
        tokenized = spacy_tokeniser.tokenise_sentence(sent)
        
        
        if(not USE_MRPC_DATA):
            if len(tokenized)> 30:
                print(f"no of tokens inthis sentence is {len(tokenized)}")
                continue
        spiders_diagram = spiders_reader.sentence2diagram(sent)
        list_target.append(spiders_diagram)
        labels_target.append(label)
    print("no. of items processed= ", len(list_target))
    return list_target, labels_target

#convert the plain text input to ZX diagrams.
train_diagrams, train_labels_v2 = spanish_diagrams(train_data,train_labels)
val_diagrams, val_labels_v2 = spanish_diagrams(val_data,val_labels)
test_diagrams, test_labels_v2 = spanish_diagrams(test_data,test_labels)

train_labels = train_labels_v2
val_labels = val_labels_v2
test_labels = test_labels_v2

"""
these are now orphan codes, but in reality, these code were there in khatri's original
code (https://colab.research.google.com/drive/13W_oktxSFMAB6m5Rfvy8vidxuQDrCWwW#scrollTo=0be9c058)
Mithun@27thsep2024-I have a bad feeling I might have removed all this an year ago, when it was giving "error"
Clearly my mental state at that time ws so messed pu that all i was trying to do it, somehow get it to work.
even if it means removing bug filled code...weird/sad but true.
-- 
from collections import Counter
# We omit any case where the 2 phrases are not parsed to the same type
joint_diagrams_train = [d1 @ d2.r if d1.cod == d2.cod else None for (d1, d2) in zip(train_diags1, train_diags2)]
joint_diagrams_test = [d1 @ d2.r if d1.cod == d2.cod else None for (d1, d2) in zip(test_diags1, test_diags2)]


train_diags_raw = [d for d in joint_diagrams_train if d is not None]
train_y = np.array([y for d,y in zip(joint_diagrams_train, filt_train_y) if d is not None])

test_diags_raw = [d for d in joint_diagrams_test if d is not None]
test_y = np.array([y for d,y in zip(joint_diagrams_test, filt_test_y) if d is not None])

print("FINAL DATASET SIZE:")
print("-----------------------------------")
print(f"Training: {len(train_diags_raw)} {Counter([tuple(elem) for elem in train_y])}")
print(f"Testing : {len(test_diags_raw)} {Counter([tuple(elem) for elem in test_y])}")

from tqdm import tqdm
from lambeq import Rewriter, remove_cups

rewriter = Rewriter(['prepositional_phrase', 'determiner', 'coordination', 'connector', 'prepositional_phrase'])

train_X = []
test_X = []

for d in tqdm(train_diags_raw):
    train_X.append(remove_cups(rewriter(d).normal_form()))

for d in tqdm(test_diags_raw):
    test_X.append(remove_cups(rewriter(d).normal_form()))

from discopy.quantum.gates import CX, Rx, H, Bra, Id

equality_comparator = (CX >> (H @ Rx(0.5)) >> (Bra(0) @ Id(1)))
equality_comparator.draw()

"""


"""
print and assert statements for debugging
"""
assert len(train_diagrams)== len(train_labels_v2)
print(len(train_diagrams), len(test_diagrams), len(val_diagrams))
assert len(train_diagrams)== len(train_labels)
assert len(val_diagrams)== len(val_labels)
assert len(test_diagrams)== len(test_labels)

def run_experiment(nlayers=1, seed=SEED):

    """mithuns comment @26thsep2024typically spider ansatz only goes with spider reader. 
    like i mentioned earlier, spider was used to just get the code off the ground
    1) we need to definitely test with atleast bobcat parser
    2) Noun should have a higher dimension than sentence? how? 
    - go back and confirm the original 1958 paper by lambek. also how
    is the code in LAMBEQ deciding the dimensions or even what  data types to use?
    answer might be in 2010 discocat paper"""
    ansatz = ansatz_to_use({AtomicType.NOUN: Dim(4),
                        AtomicType.SENTENCE: Dim(2)
                        #    AtomicType.PREPOSITIONAL_PHRASE: Dim(2),
                        })
    
    """
    todo: his original code for ansatz is as below. Todo find out: why we switched to the above.
    I think it had something do with spider ansatz-
    todo: write the whole history of playing with this code- why spider ansatz etc- in one single
    word document, like chronological order.-for your own sanity

N = AtomicType.NOUN
S = AtomicType.SENTENCE
P = AtomicType.PREPOSITIONAL_PHRASE
print(f'RUNNING WITH {nlayers} layers')
    ansatz = Sim15Ansatz({N: 1, S: 1, P:1}, n_layers=nlayers, n_single_qubit_params=3)

    Also the two lines below is more to do with comparing two things, like NLI/MRPC, Might not be that
    relevant in say classification

    train_circs = [ansatz(d) >> equality_comparator for d in train_X]
    test_circs = [ansatz(d) >> equality_comparator for d in test_X]
    """

    #use the anstax to create circuits from diagrams
    train_circuits =  [ansatz(diagram) for diagram in train_diagrams]
    val_circuits =  [ansatz(diagram) for diagram in val_diagrams]
    test_circuits = [ansatz(diagram) for diagram in test_diagrams]


    from lambeq import PytorchModel

    #mithuns comment @26thsep2024:pytorch model was the only one going well
    #  with spider reader and spider anstaz
    #also todo: should this not contain both val and train circuits as inputz?
    qnlp_model = PytorchModel.from_diagrams(train_circuits)

    

    print([len(x) for x in train_circuits])

    train_dataset = Dataset(
                train_circuits,
                train_labels,
                batch_size=BATCH_SIZE)

    val_dataset = Dataset(val_circuits, val_labels, shuffle=False)

    print(len(train_labels), len(train_circuits))
    #print and assert statements for debugging
    print(len(train_circuits), len(val_circuits), len(test_circuits))
    assert len(train_circuits)== len(train_labels)
    assert len(val_circuits)== len(val_labels)
    assert len(test_circuits)== len(test_labels)

    """
    #this is a;;wandb crap,- i was trying to automate a sweep across parameters. 
    Turned out to be more  hurting than helping. Will think about this as and when we get to expt running stage
    # comment out if becoming a pain in the butt. MLFlow is better than wandb imho
    sweep_config = {
        'method': 'random'
        }
    metric = {
        'name': 'loss',
        'goal': 'minimize'
        }

    sweep_config['metric'] = metric

    parameters_dict = {
        'LEARNING_RATE': {
            'values': [0.3, 0.03, 0.003,0.0003]
            },
        }

    sweep_config['parameters'] = parameters_dict

    parameters_dict.update({
        'epochs': {
            'value': 1}
        })

    import pprint
    
    pprint.pprint(sweep_config)
    # sweep_id = wandb.sweep(sweep_config, project="uspantekan_spider_tuning")
    # wandb_logger = WandbLogger()"""

    trainer = PytorchTrainer(
            model=qnlp_model,
            loss_function=torch.nn.BCEWithLogitsLoss(),
            optimizer=torch.optim.AdamW,
            learning_rate=LEARNING_RATE,
            epochs=EPOCHS,
            evaluate_functions=eval_metrics,
            evaluate_on_train=True,
            verbose='text',
            seed=SEED)



    # train_preds = qnlp_model.get_diagram_output(train_circuits)
    # loss_pyTorch = nn.CrossEntropyLoss()
    # loss_pyTorch(train_preds, train_labels)
    # print(f"TRAIN STATS: {loss_pyTorch}")

    # #todo add f1 also

    # train_loss = loss(train_preds, train_labels)
    # train_acc = acc(train_preds, train_labels)
    # print(f'TRAIN STATS: {train_loss, train_acc}')

    

    train_embeddings, test_embeddings, max_w_param_length = generate_initial_parameterisation(train_circuits, val_circuits, embedding_model, qnlp_model)

    #run ONLY the QNLP model.i.e let it train on the train_dataset. and test on val_dataset
    #todo:  somewhere you use the term val and somehwere else you use the term test.
    #  Fix it/use only one everywhere-bottomline: make val/dev explicitly different than test

    trainer.fit(train_dataset, val_dataset, eval_interval=1, log_interval=1)


    """by here the actual QNLP model is trained. Next is we are going to connect the model which learns
    #  relationsihp between fasttext emb and angles. look inside the function
    generate_initial_parameterisation for more specific comments

    
    #now we get into the land of model 3- i.e the model which learns the connection between weights of model 1 
    i.e the QNLP model. 
    and embeddings of model 2
    i.e model 3=model(a,b) where a = embeddings and b = angles
    todo: find the difference between angles nad weights- why is it per word? """

    NN_model = generate_OOV_parameterising_model(qnlp_model, train_embeddings, max_w_param_length)

    #this is model 4, a separate model initialized based only on test_circuits
    #todo: find why does he have two models instead of using the oringal qnlp_model to do .predict()
    #note/todo: differentiate difference between test and val. If it is val here, then make sure we are not passing val inside model1.fit()
    prediction_model = PytorchModel.from_diagrams(test_circuits)

    """#Create a dictionary where key is word and weights are the trained weights after QNLP_model was
    trained. 
    Todo: What I dont understand is, why is he doing that agian here? this is exactly what he is doing
    in the first few lines of  generate_OOV_parameterising_model. 
    a) check if this is my mistake or its same in his original code
    b) debug and check if values are same. i.e in generate_oov_* and here. if yes, return from generate_oov_* 
    """
    trained_wts = trained_params_from_model(qnlp_model, train_embeddings, max_w_param_length)


    
    smart_loss, smart_acc = evaluate_test_set(prediction_model,
                                                test_circuits,
                                                test_labels,
                                                trained_wts,
                                                test_embeddings,
                                                max_w_param_length,
                                                OOV_strategy='model',
                                                OOV_model=NN_model)


