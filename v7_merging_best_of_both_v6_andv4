# -*- coding: utf-8 -*-
"""v7
#name: v7_*
# status: this version stops at .fit()- rather some bug. v4 has my code which ran end to end- atleast passes .fit(). s
#will create a new file v7 which takes best of both worlds.
"""


import wandb
# wandb.init(project="v4_uspantekan")

import torch
from torch import nn
import wandb
import spacy
from lambeq import SpacyTokeniser
import numpy as np
import fasttext as ft
from lambeq import PytorchTrainer
from lightning.pytorch.loggers import WandbLogger
from lambeq.backend.tensor import Dim
from lambeq import AtomicType, SpiderAnsatz

ansatz_to_use = SpiderAnsatz
embedding_model = ft.load_model('./embeddings-l-model.bin')


BATCH_SIZE = 30
EPOCHS = 2
LEARNING_RATE = 0.05
SEED = 0
DATA_BASE_FOLDER= "data"

USE_MRPC_DATA=False
USE_SPANISH_DATA=True
USE_USP_DATA=False

if(USE_USP_DATA):
    TRAIN="uspantek_train.txt"
    DEV="uspantek_dev.txt"
    TEST="uspantek_test.txt"

if(USE_SPANISH_DATA):
    TRAIN="spanish_train.txt"
    DEV="spanish_dev.txt"
    TEST="spanish_test.txt"


if(USE_MRPC_DATA):
    TRAIN="mrpc_train_80_sent.txt"
    DEV="mrpc_dev_10_sent.txt"
    TEST="mrpc_test_10sent.txt"


spacy_tokeniser = SpacyTokeniser()

if(USE_SPANISH_DATA) or (USE_USP_DATA):
    spanish_tokeniser=spacy.load("es_core_news_sm")
    spacy_tokeniser.tokeniser = spanish_tokeniser



#for english tokenizer
if(USE_MRPC_DATA):
    english_tokenizer = spacy.load("en_core_web_sm")
    spacy_tokeniser.tokeniser =english_tokenizer


import os
from lambeq import BobcatParser

#the initial phases of the gates
def generate_initial_parameterisation(train_circuits, test_circuits, embedding_model, qnlp_model):

    # extract the words from the circuits- i.e the training data
    # Note that in this vocab, the same word can have multiple types, which each occur separately
    # todo: what did he mean by same word having multiple types. is this the likes vs never example mentione din 1958 lambek paper?
    train_vocab = {symb.name.rsplit('_', 1)[0] for d in train_circuits for symb in d.free_symbols}
    test_vocab = {symb.name.rsplit('_', 1)[0] for d in test_circuits for symb in d.free_symbols}

    
    #todo print: the total number of words in train, and test+ note it down
    #.answer for spanish henderson there are 463 words in training and 89 in testing
    # out of the 89 words in test, 33 are not present in training, so they are OOV
    print(len(test_vocab.union(train_vocab)), len(train_vocab), len(test_vocab))    
    print(f"OOV word count: i.e out of {len(test_vocab)} there are  {len(test_vocab - train_vocab)} words that are not found in training. So they are OOV")

    #todo: find the meaning of symbol count- what is the difference between symbol count and OOV or train_vocab
    n_oov_symbs = len({symb.name for d in test_circuits for symb in d.free_symbols} - {symb.name for d in train_circuits for symb in d.free_symbols})
    print(f'OOV symbol count: {n_oov_symbs} / {len({symb.name for d in test_circuits for symb in d.free_symbols})}')

    def get_max_word_param_length(input_circuits):
        lengths=[]
        for d in input_circuits:
            for symb in d.free_symbols:
                x =  symb.name.split('_', 1)[1]
                y = x.split('__')[0]
                lengths.append(int(y))
        return lengths
    
    max_word_param_length=0
    if(ansatz_to_use==SpiderAnsatz):
        max_word_param_length_train = max(get_max_word_param_length(train_circuits))
        max_word_param_length_test = max(get_max_word_param_length(test_circuits))
        max_word_param_length = max(max_word_param_length_train, max_word_param_length_test) + 1

    assert max_word_param_length!=0
    
    # max_word_param_length = max(max(int(symb.name.rsplit('_', 1)[1]) for d in train_circuits for symb in d.free_symbols),
    #                         max(int(symb.name.rsplit('_', 1)[1]) for d in test_circuits for symb in d.free_symbols)) + 1
    

    # max_word_param_length = max(, ) + 1
    print(f'Max params/word: {max_word_param_length}')

    #next , for each word in train and test vocab , we need to get its embedding from fasttext
    
    if(ansatz_to_use==SpiderAnsatz):  
        # train_vocab_embeddings={}      
        def get_vocab_emb_dict(vocab):
            embed_dict={}
            for wrd in vocab:
                #spider ansatz alone writes the tokens in its vocabulary with a single underscore first and then a double underscore
                
                #todo: some words are already in dictionary- i think this is because of the same words having multiple versions- mostly likely we shouldn't split the _1_ thing- i am thinking 
                #that denotes the nth TYPE of LIKES kinda adverbs.
                cleaned_wrd=wrd.split('_')[0].replace('\\','').replace(",","")
                if cleaned_wrd in embed_dict   :
                    print(f"error.  the word {cleaned_wrd} was already in dict")
                else:
                    embed_dict[cleaned_wrd]= embedding_model[cleaned_wrd] 
            return embed_dict

        #for each word in train and test vocab get its embedding from fasttext
        #note that even though the symbols per se have _0_, in the train_vocab_embedding dictionary it is stored in the
        #format of {wrd: embedding}- i.e only the word aldea out of aldea_0_ is separated out and used.
        train_vocab_embeddings = get_vocab_emb_dict(train_vocab)
        test_vocab_embeddings = get_vocab_emb_dict(test_vocab)
    
        

    else:
        #for the words created from other ansatz just write it as : _0_ so we can reuse the parsing from original khatri code. But here recording specifically for this instance
        train_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in train_vocab}
        test_vocab_embeddings = {wrd: embedding_model[wrd.split('__')[0]] for wrd in test_vocab}


    #to store all the initial weights
    initial_param_vector = []

    #todo: find what qnlp_model.symbols is- rather how is it different than train vocab?
    for sym in qnlp_model.symbols:
        #@sep2nd2024-not sure what idx is supposed to do, am giong to give it the number associated with the word
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  sym.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = sym.name.split('_', 1)[1]
            idx = rest.split('__')[0]      
            #@sep2nd2024/ end of day: getting key error for lots of words - e.g. aldea..but why are words 
            # in qnlpmodel.symbols not getting the fasttext emb on the fly? why are we separating train_embeddings earlier?        
            #what is the meaning of symbols in qnlp.model
            #todo a) read the lambeq documentation on symbols
            #  b) read the 2010 discocat and CQM paper onwards up, chronologically
            #no point turning knobs without deeply understanding what symbols do
            #todo:compare the format ith mrpc data, and see if he is storing the initial param vector- and symbols with _0_ or not?
            if wrd in train_vocab_embeddings:
                initial_param_vector.append(train_vocab_embeddings[wrd][int(idx)])
            else:
                #todo: lots of words are getting hit with OOV- conirm why they are not there in fasttext emb
                # my guess is its all the unicode characters. In theory fast text is meant to create zero OOV..since it builds up from 1 gram 2 gram etc
                #update: this might be caused because am removing the _0_ thing from the actual name, without realizing what it is doing.
                '''
                found that this word verdad, was OOV/not in fasttext emb
                found that this word viÃ³, was OOV/not in fasttext emb
                found that this word yo, was OOV/not in fasttext emb
                found that this word yyyyyy was OOV/not in fasttext emb
                '''
                print(f"found that this word {wrd} was OOV/not in fasttext emb")

    qnlp_model.weights = nn.ParameterList(initial_param_vector)

    return train_vocab_embeddings, test_vocab_embeddings, max_word_param_length

def retrieve_wrd_from_symbol_for_spider_ansatz(ansatz_to_use,sym):
    if(ansatz_to_use==SpiderAnsatz):  
            wrd =  sym.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = sym.name.split('_', 1)[1]
            idx = rest.split('__')[0]      
            #@sep2nd2024/ end of day: getting key error for lots of words - e.g. aldea..but why are words 
            # in qnlpmodel.symbols not getting the fasttext emb on the fly? why are we separating train_embeddings earlier?        
            #what is the meaning of symbols in qnlp.model
            #todo a) read the lambeq documentation on symbols
            #  b) read the 2010 discocat and CQM paper onwards up, chronologically
            #no point turning knobs without deeply understanding what symbols do
            #todo:compare the format ith mrpc data, and see if he is storing the initial param vector- and symbols with _0_ or not?
            if wrd in train_vocab_embeddings:
                initial_param_vector.append(train_vocab_embeddings[wrd][int(idx)])

def generate_OOV_parameterising_model(trained_qnlp_model, train_vocab_embeddings, max_word_param_length):
    """Read arguments from command line.

    Args:
    trained_qnlp_model- the trained_qnlp_model
    train_vocab_embeddings- the initial embeddings for words in the vocab got from fasttext
    max_word_param_length- what is the maximum size of a word

    Returns:
    Weights of a NN model which now understands/has weights for each word in fasttext as its original embedding influenced/mapped to the weights in the trained QNLP model

    """

    
    trained_params_raw = {symbol: param for symbol, param in zip(trained_qnlp_model.symbols, trained_qnlp_model.weights)}

    """explanation of above code:
    trained_params_raw is dictionary that map words in the trained QNLP model to 
    its weights at the end of QNLP training. note that currently/initially the weights are 
    the initial fast text embedding values- we are just initializing it using a standard set of 
    values /embeddings-which will then be updated as the multi layer perceptron learns the connection
    or mapping or relationship between fast text embeddings, and the angles learned by the QNLP model
    
    #todo: print and confirm if symbol means word
    #update@sep 11th 2024 : symbol is not word, it is word+ that idx number- 
    # which i am suspecting is the number of Types a same word can have
    # for example if you read 1958 Lambek paper youc an see that adverb Likes can have two different TYPE representations.
    # but yes, todo: confirm this by comparing it with the original Khatri MRPC code."""


    print(trained_params_raw)
    
    
    trained_param_vectors = {wrd: np.zeros(max_word_param_length) for wrd in train_vocab_embeddings}
    """
    # train_vocab_embeddings are the initial embeddings 
    for words in the vocab we got from fasttext- for each such word create an array of zeroes
    - this array is where the weights of the MLP will be added, i think?

    #i.e for now for each such word in training vocabulary, create a vector filled with zeroes to represent
    #trained parameters (todo: what exaclty is trained parameters- is it same as weights of the MLP 
    #neural network?) whose size will be the max size of sentence in training
    #todo: is this being done for padding?

    """
    print(trained_param_vectors)
    
    '''for each such symbol and parameter weight assign the weights
      to the empty array/array  array created above.-
    #todo print and confirm, i think they are repeating the same
    #  weight value for every entry of the array in trained_param_vectors'''
    for symbol, train_val in trained_params_raw.items():
        if(ansatz_to_use==SpiderAnsatz):  
            wrd =  symbol.name.split('_', 1)[0].replace("\\","").replace("(","")
            rest = symbol.name.split('_', 1)[1]
            idx = rest.split('__')[0]    
        else:
            wrd, idx = symbol.name.rsplit('_', 1)
        trained_param_vectors[wrd][int(idx)] = train_val

    wrds_in_order = list(train_vocab_embeddings.keys())

    #so the value to be trained now are the initial weights of each word from
    #fasttext, which will be trained against  gold label -trained_param_vectors i.e weights from the trained QNLP model
    NN_train_X = np.array([train_vocab_embeddings[wrd] for wrd in wrds_in_order])
    NN_train_Y = np.array([trained_param_vectors[wrd] for wrd in wrds_in_order])

    print(NN_train_X[0][:5])
    print(NN_train_Y[0][:5])

    OOV_NN_model = keras.Sequential([
      layers.Dense(int((max_word_param_length + MAXPARAMS) / 2), activation='tanh'),
      layers.Dense(max_word_param_length, activation='tanh'),
    ])

    OOV_NN_model.compile(loss='mean_absolute_error', optimizer=keras.optimizers.Adam(0.001))

    # Embedding dim!
    OOV_NN_model.build(input_shape=(None, MAXPARAMS))

    hist = OOV_NN_model.fit(NN_train_X, NN_train_Y, validation_split=0.2, verbose=0, epochs=120)

    print(f'OOV NN model final epoch loss: {(hist.history["loss"][-1], hist.history["val_loss"][-1])}')

    plt.plot(hist.history['loss'], label='loss')
    plt.plot(hist.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.show()

    return OOV_NN_model

def read_data(filename):
    labels, sentences = [], []
    with open(filename) as f:
        for line in f:
            t = float(line[0])
            labels.append([t, 1-t])
            sentences.append(line[1:].strip())
    return labels, sentences

train_labels, train_data = read_data(os.path.join(DATA_BASE_FOLDER,TRAIN))
val_labels, val_data = read_data(os.path.join(DATA_BASE_FOLDER,DEV))
test_labels, test_data = read_data(os.path.join(DATA_BASE_FOLDER,TEST))

TESTING = int(os.environ.get('TEST_NOTEBOOKS', '0'))

if TESTING:
    train_labels, train_data = train_labels[:2], train_data[:2]
    val_labels, val_data = val_labels[:2], val_data[:2]
    test_labels, test_data = test_labels[:2], test_data[:2]
    EPOCHS = 1

train_data[:5]

train_labels[:5]

from lambeq import spiders_reader
#
#parser = BobcatParser(verbose='text')

from tqdm import tqdm
def spanish_diagrams(list_sents,labels):
    list_target = []
    labels_target = []
    for sent, label in tqdm(zip(list_sents, labels),desc="reading sent"):
        #using bob cat parser- note: this wasn't compatible with spider ansatz
        # tokenized = spacy_spanish_tokeniser.tokenise_sentence(sent)
        # diag =parser.sentence2diagram(tokenized, tokenised= True)
        # diag.draw()
        # list_target.append(diag)
        
        if( USE_MRPC_DATA):
            sent = sent.split('\t')[2]
        tokenized = spacy_tokeniser.tokenise_sentence(sent)
        
        
        if(not USE_MRPC_DATA):
            if len(tokenized)> 30:
                print(f"no of tokens inthis sentence is {len(tokenized)}")
                continue
        spiders_diagram = spiders_reader.sentence2diagram(sent)
        list_target.append(spiders_diagram)
        labels_target.append(label)
    print("no. of items processed= ", len(list_target))
    return list_target, labels_target

train_diagrams, train_labels_v2 = spanish_diagrams(train_data,train_labels)
val_diagrams, val_labels_v2 = spanish_diagrams(val_data,val_labels)
test_diagrams, test_labels_v2 = spanish_diagrams(test_data,test_labels)

train_labels = train_labels_v2
val_labels = val_labels_v2
test_labels = test_labels_v2

"""val_diagrams = spanish_diagrams(val_data)<br>
test_diagrams = spanish_diagrams(test_data)

rint and assert statements for debugging
"""

assert len(train_diagrams)== len(train_labels_v2)
print(len(train_diagrams), len(test_diagrams), len(val_diagrams))
assert len(train_diagrams)== len(train_labels)
assert len(val_diagrams)== len(val_labels)
assert len(test_diagrams)== len(test_labels)
# train_diagrams[0].draw()
# val_diagrams[0].draw()
# test_diagrams[0].draw()



ansatz = ansatz_to_use({AtomicType.NOUN: Dim(4),
                       AtomicType.SENTENCE: Dim(2)
                    #    AtomicType.PREPOSITIONAL_PHRASE: Dim(2),
                       })

train_circuits =  [ansatz(diagram) for diagram in train_diagrams]
val_circuits =  [ansatz(diagram) for diagram in val_diagrams]
test_circuits = [ansatz(diagram) for diagram in test_diagrams]

# train_circuits[0].draw()

from lambeq import PytorchModel

all_circuits = train_circuits + val_circuits + test_circuits
qnlp_model = PytorchModel.from_diagrams(all_circuits)

sig = torch.sigmoid

def accuracy(y_hat, y):
    return torch.sum(torch.eq(torch.round(sig(y_hat)), y))/len(y)/2  # half due to double-counting

eval_metrics = {"acc": accuracy}

from lambeq import Dataset

print([len(x) for x in train_circuits])

train_dataset = Dataset(
            train_circuits,
            train_labels,
            batch_size=BATCH_SIZE)

val_dataset = Dataset(val_circuits, val_labels, shuffle=False)

print(len(train_labels), len(train_circuits))
#print and assert statements for debugging
print(len(train_circuits), len(val_circuits), len(test_circuits))
assert len(train_circuits)== len(train_labels)
assert len(val_circuits)== len(val_labels)
assert len(test_circuits)== len(test_labels)
# train_circuits[0].draw()
# val_circuits[0].draw()
# test_circuits[0].draw()

sweep_config = {
    'method': 'random'
    }
metric = {
    'name': 'loss',
    'goal': 'minimize'
    }

sweep_config['metric'] = metric

parameters_dict = {
    'LEARNING_RATE': {
          'values': [0.3, 0.03, 0.003,0.0003]
        },
    }

sweep_config['parameters'] = parameters_dict

parameters_dict.update({
    'epochs': {
        'value': 1}
    })

import pprint

pprint.pprint(sweep_config)
# sweep_id = wandb.sweep(sweep_config, project="uspantekan_spider_tuning")







# wandb_logger = WandbLogger()
trainer = PytorchTrainer(
        model=qnlp_model,
        loss_function=torch.nn.BCEWithLogitsLoss(),
        optimizer=torch.optim.AdamW,
        learning_rate=LEARNING_RATE,
        epochs=EPOCHS,
        evaluate_functions=eval_metrics,
        evaluate_on_train=True,
        verbose='text',
        seed=SEED)
trainer.fit(train_dataset, val_dataset, eval_interval=1, log_interval=1)

# by here the actual QNLP model is trained. Next is we are going to connect the model which learns relationsihp between fasttext emb and angles
train_embeddings, test_embeddings, max_w_param_length = generate_initial_parameterisation(train_circuits, val_circuits, embedding_model, qnlp_model)

print('BEGINNING DNN MODEL TRAINING')
NN_model = generate_OOV_parameterising_model(qnlp_model, train_embeddings, max_w_param_length)

   


